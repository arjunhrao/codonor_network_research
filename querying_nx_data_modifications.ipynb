{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting program\n",
      "212\n",
      "22366\n",
      "1.0\n",
      "['ROBERT ANDREWS', 'BERNARD SANDERS', 'GEORGE ALLEN', 'SPENCER BACHUS', 'EDWARD ROYCE', 'PETER HOEKSTRA', 'ROBERT MENENDEZ', 'MIKE COFFMAN', 'TOM LATHAM', 'LLOYD DOGGETT', 'JESSE JACKSON', 'DIANA DEGETTE', 'JAMES MCGOVERN', 'JOHN TIERNEY', 'DEBBIE STABENOW', 'JO EMERSON', 'WILLIAM PASCRELL', 'CAROLYN MCCARTHY', 'MIKE MCINTYRE', 'RONALD KIND', 'LOIS CAPPS', 'BARBARA LEE', 'HEATHER WILSON', 'JUDY BIGGERT', 'SHELLEY BERKLEY', 'RUSH HOLT', 'PAUL RYAN', 'TAMMY BALDWIN', 'MARIA CANTWELL', 'DIANNE FEINSTEIN', 'DANIEL LUNGREN', 'BILL NELSON', 'DAVID GILL', 'STENY HOYER', 'THOMAS CARPER', 'MARCY KAPTUR', 'BENJAMIN CARDIN', 'PETER DEFAZIO', 'LOUISE SLAUGHTER', 'FREDERICK UPTON', 'NANCY PELOSI', 'NITA LOWEY', 'ILEANA ROS-LEHTINEN', 'JEFFRY FLAKE', 'MICHAEL HONDA', 'TODD AKIN', 'DENNIS REHBERG', 'STEVE ISRAEL', 'PATRICK TIBERI', 'ERIC CANTOR', 'SHELLEY CAPITO', 'RAUL GRIJALVA', 'STEVEN KING', 'BEN CHANDLER', 'SCOTT GARRETT', 'TIMOTHY BISHOP', 'TIM MURPHY', 'THOMAS HENSARLING', 'CONNIE MACK', 'DEBBIE WASSERMAN SCHULTZ', 'THOMAS PRICE', 'JOHN BARROW', 'JR BOUSTANY', 'JERRY MCNERNEY', 'JOSEPH DONNELLY', 'BETTY SUTTON', 'DONNA EDWARDS', 'ALAN GRAYSON', 'DANIEL MAFFEI', 'PETER ROSKAM', 'AMY KLOBUCHAR', 'CLAIRE MCCASKILL', 'JON TESTER', 'ROBERT CASEY', 'SHELDON WHITEHOUSE', 'L. DUCKWORTH', 'KEVIN MCCARTHY', 'CHRISTOPHER MURPHY', 'VERNON BUCHANAN', 'MAZIE HIRONO', 'BRUCE BRALEY', 'DAVID LOEBSACK', 'TIMOTHY WALZ', 'KEITH ELLISON', 'DEAN HELLER', 'CAROL SHEA-PORTER', 'G. FOSTER', 'NICOLA TSONGAS', 'ANN KIRKPATRICK', 'JARED POLIS', 'JIM HIMES', 'GARY PETERS', 'ERIK PAULSEN', 'MARTIN HEINRICH', 'PAUL TONKO', 'KURT SCHRADER', 'GERRY CONNOLLY', 'ANDREW HARRIS', 'JOE GARCIA', 'JUDY CHU', 'WILLIAM OWENS', 'JOHN GARAMENDI', 'MARK CRITZ', 'JEFF DENHAM', 'WILLIAM SOUTHERLAND', 'RAYMOND CRAVAACK', 'NAN HAYWORTH', 'ANN BUERKLE', 'JAMES RENACCI', 'PATRICK MEEHAN', 'DAVID CICILLINE', 'KRISTI NOEM', 'ROBERT HURT', 'SEAN DUFFY', 'REID RIBBLE', 'SCOTT BROWN', 'AMERISH BERA', 'KAREN HARRINGTON', 'GARY MCDOWELL', 'ANN KUSTER', 'RANDOLPH ALTSCHULER', 'MANAN TRIVEDI', 'DENNIS HECK', 'SUZAN DELBENE', 'PEDRO PIERLUISI', 'RICHARD CARMONA', 'LINDA LINGLE', 'ELIZABETH WARREN', 'ANGUS KING', 'HEIDI HEITKAMP', 'DEBRA FISCHER', 'JOSEPH KYRILLOS', 'TOM SMITH', 'RAFAEL CRUZ', 'TIMOTHY KAINE', 'TOMMY THOMPSON', 'SCOTT TIPTON', 'ALLEN WEST', 'ROBERT DOLD', 'TIMOTHY SCOTT', 'FRANCISCO CANSECO', 'KATHLEEN HOCHUL', 'JANICE HAHN', 'WILLIAM ENYART', 'TOBIAS SCHLINGENSIEPEN', 'JOE MANCHIN', 'RICHARD NOLAN', 'RICHARD MOURDOCK', 'GARLAND BARR', 'THOMAS COTTON', 'RONALD BARBER', 'KYRSTEN SINEMA', 'ALAN LOWENTHAL', 'JULIA BROWNLEY', 'JOSE HERNANDEZ', 'RICKY GILL', 'RAUL RUIZ', 'MARK TAKANO', 'SCOTT PETERS', 'SALVATORE PACE', 'BRANDON SHAFFER', 'JOE MIKLOSI', 'ELIZABETH ESTY', 'ANDREW RORABACK', 'VALDEZ DEMINGS', 'KEITH FITZGERALD', 'LOIS FRANKEL', 'PATRICK MURPHY', 'ADAM HASNER', 'CHRISTIE VILSACK', 'NICOLE LEFAVOUR', 'BRADLEY SCHNEIDER', 'CHERI BUSTOS', 'BRENDAN MULLEN', 'SHELLI YODER', 'RICHARD TISEI', 'JOSEPH KENNEDY', 'JOHN DELANEY', 'DANIEL KILDEE', 'JAMES GRAVES', 'ANN WAGNER', 'PAM GULLESON', 'SHELLEY ADLER', 'JOHN OCEGUERA', 'STEVEN HORSFORD', 'JULIAN SCHREIBMAN', 'GRACE MENG', 'HAKEEM JEFFRIES', 'MARK MURPHY', 'NATHAN SHINAGAWA', 'SEAN MALONEY', 'JOYCE BEATTY', 'ANGELA ZIMMANN', 'ROB WALLACE', 'SUZANNE BONAMICI', 'TOM RICE', 'MATT VARILEK', 'PETE GALLEGO', 'ROGER WILLIAMS', 'MIA LOVE', 'PAUL HIRSCHBIEL', 'ERNEST POWELL', 'JOHN DOUGLASS', 'DEREK KILMER', 'ROB ZERBAN', 'MARK POCAN', 'PATRICK KREITLOW', 'SUSAN THORN', 'PATRICIA KEEVER', 'ROBERT KERREY', 'MICHELE BACHMANN', 'KIRSTEN GILLIBRAND']\n"
     ]
    }
   ],
   "source": [
    "#read graph as edge list\n",
    "print('starting program')\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "#codonor graph - nodes are recipients. Edges are number of donors in common.\n",
    "H1 = nx.read_edgelist('co_donor_relabeled_nodes.txt',nodetype=str,delimiter='\\t!\\t')#data=(('number_of_codonors',int)))\n",
    "\n",
    "print(nx.number_of_nodes(H1) )#should be 212\n",
    "print( nx.number_of_edges(H1) )#should be 22366\n",
    "print(nx.density(H1) )\n",
    "\n",
    "print(H1.nodes()) #[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a list of the number of codonors\n",
    "num_codonors_list = []\n",
    "for e in H1.edges(data=True):\n",
    "    #e is a tuple\n",
    "    temp = (e[2])['number_of_codonors']\n",
    "    num_codonors_list.append(temp)\n",
    "#print(num_codonors_list)\n",
    "\n",
    "#get a list of the edges and sort by num codonors\n",
    "list_of_edges = []\n",
    "for e in H1.edges(data=True):\n",
    "    list_of_edges.append(e)\n",
    "list_of_edges = sorted(H1.edges(data=True), key=lambda tup: (tup[2]['number_of_codonors'],tup[1]) )#sort by number of codonors\n",
    "#should sort by the name of first node of edge one after this for consistency.\n",
    "\n",
    "\n",
    "#print(list_of_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_edges:\n",
      "22366\n",
      "1118.3\n",
      "high_bkt len:\n",
      "1118\n",
      "mid_bkt len:\n",
      "5591\n",
      "low_bkt len:\n",
      "15657\n"
     ]
    }
   ],
   "source": [
    "num_edges = len(list_of_edges)\n",
    "print(\"num_edges:\")\n",
    "print(num_edges)\n",
    "print(float(.05*num_edges))\n",
    "high_bkt = []#all edges above the 5% mark\n",
    "high_cutoff = num_edges - float(.05*num_edges)\n",
    "mid_bkt = []\n",
    "mid_cutoff = num_edges - float(.3*num_edges)\n",
    "low_bkt = []\n",
    "\n",
    "for e in range(num_edges):\n",
    "    if e >= high_cutoff:\n",
    "        high_bkt.append(list_of_edges[e])\n",
    "    elif e >= mid_cutoff:\n",
    "        mid_bkt.append(list_of_edges[e])\n",
    "    else:\n",
    "        low_bkt.append(list_of_edges[e])\n",
    "        \n",
    "#print(high_bkt)\n",
    "print(\"high_bkt len:\")\n",
    "print(len(high_bkt))\n",
    "print(\"mid_bkt len:\")\n",
    "print(len(mid_bkt))\n",
    "print(\"low_bkt len:\")\n",
    "print(len(low_bkt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm just going to do the entire high bucket as opposed to sampling from the lower ones because in the past the lower ones have led to poor results - at least in Google CSE, the values were often 0, which is undesirable. Doing the entire bucket as opposed to randomly sampling also allows me to not to overlap the edges I query on Google. I can always randomly sample from the bucket afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#get dictionary. Implement logic behind checking and saving within scraping function\n",
    "#https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "names_to_results_dict = {}\n",
    "with open('names_to_results_dict.pickle', 'rb') as handle:\n",
    "    names_to_results_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#host google.com 8.8.8.8\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#had to write a scraping method to work for me until I figure out why I can't find the server...\n",
    "def jaccard_google_search_scrape(search_term1, search_term2, api_key, cse_id, **kwargs):\n",
    "\n",
    "    num1 = -2\n",
    "    num2 = -2\n",
    "    #only scrape if it's not in the dict\n",
    "    if (names_to_results_dict.get(search_term1[1:-1]) is None):\n",
    "        r = requests.get(\"https://www.google.com/search\", params={'q':search_term1})\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "        if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "            num1 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "        else:\n",
    "            num1 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict[search_term1[1:-1]] = num1\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term1\")\n",
    "    else:\n",
    "        num1 = names_to_results_dict[search_term1[1:-1]]\n",
    "\n",
    "    if (names_to_results_dict.get(search_term2[1:-1]) is None):\n",
    "        r = requests.get(\"https://www.google.com/search\", params={'q':search_term2})\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "\n",
    "        if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "            num2 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "        else:\n",
    "            num2 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict[search_term2[1:-1]] = num2\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term2\")\n",
    "    else:\n",
    "        num2 = names_to_results_dict[search_term2[1:-1]]\n",
    "        \n",
    "    #the dict will keep updating as necessary during a set of queries, but must make sure to store before exiting\n",
    "    #in order to maintain the values we added. Since this exit could happen at any query, may as well save pickle every\n",
    "    #time we add to the dict.\n",
    "    \n",
    "    strBoth = search_term1 + ' ' + search_term2#I think this should be fine. There's so much noise anyways,\n",
    "    #and sometimes it increases the number of results, sometimes decreases, so using AND isn't better.\n",
    "    \n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':strBoth})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    numCommon = -2\n",
    "    if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "    else:\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "    \n",
    "    num1 = int(num1)\n",
    "    num2 = int(num2)\n",
    "    numCommon = int(numCommon)\n",
    "    #print('num results for str1:' + str(num1) )\n",
    "    #print('num results for str2:' + str(num2) )\n",
    "    #print('num results in common:' + str(numCommon) )\n",
    "    \n",
    "    \n",
    "    #sometimes denominator was 0. try except ZeroDivionError.\n",
    "    try:\n",
    "        jacInd = numCommon /(num1+num2-numCommon) #can still use / for regular division, but want decimals here.\n",
    "    except ZeroDivisionError:\n",
    "        jacInd = -1\n",
    "    \n",
    "    try:\n",
    "        divMin = numCommon / min(num1, num2)\n",
    "    except ZeroDivisionError:\n",
    "        divMin = -1\n",
    "    try:\n",
    "        divProd = numCommon / (num1*num2)\n",
    "    except:\n",
    "        divProd = -1\n",
    "    #retList is of format: num results for str1, num results for str2, num results in common, the Jaccard index,\n",
    "    #the intersection / the min, and the intersection / the product\n",
    "    retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    return retList\n",
    "\n",
    "print(\"method processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instanceNum: 0\n",
      "start index: 550\n",
      "end index: 570\n",
      "first edge to sample:\n",
      "('RAUL RUIZ', 'JAMES GRAVES', {'number_of_codonors': 1442})\n"
     ]
    }
   ],
   "source": [
    "#Figure out which edges to sample\n",
    "instanceNum = -1#same thing as like a machineNum\n",
    "print(\"instanceNum: \" + str(instanceNum))\n",
    "start = 570\n",
    "end = start+90\n",
    "print(\"start index: \" + str(start))\n",
    "print(\"end index: \" + str(end))\n",
    "#start = instanceNum*33\n",
    "#end = start + 33#we can technically do 33 edges safely\n",
    "#Note that if instanceNum were 1, we'd have [33:66].\n",
    "all_edges_to_sample = high_bkt[start:end]#partition based on machine number\n",
    "print(\"first edge to sample:\")\n",
    "print(all_edges_to_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "queries done\n"
     ]
    }
   ],
   "source": [
    "#space out requests more - want to write a cron script eventually for maybe ~6-8 requests per hour as opposed\n",
    "#to 100 per day which means 33 edges per day\n",
    "import time\n",
    "#time.sleep(1)\n",
    "\n",
    "\n",
    "#choose the first of the following two lines if it's the very start / origination of the file. Otherwise go with second\n",
    "#don't forget to save csv at the end\n",
    "#dfRes = pd.DataFrame(columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\"])\n",
    "dfRes = pd.read_csv('./data/codonor_network_queries_instance_0_5.21_highbkt_edges.csv',index_col=0)\n",
    "#^index_col: https://stackoverflow.com/questions/36519086/pandas-how-to-get-rid-of-unnamed-column-in-a-dataframe\n",
    "\n",
    "counter = start\n",
    "savestr = './data/codonor_network_queries_instance_0_5.21_highbkt_edges.csv'\n",
    "#'''\n",
    "try:\n",
    "    for edge in all_edges_to_sample:\n",
    "        time.sleep(1)\n",
    "        print(counter)\n",
    "        counter = counter + 1\n",
    "        name1 = '\"' + edge[0] + '\"'\n",
    "        name2 = '\"' + edge[1] + '\"'\n",
    "\n",
    "        my_api_key, my_cse_id = \"0\", \"0\"\n",
    "        res = jaccard_google_search_scrape(name1, name2, my_api_key, my_cse_id, num=1)\n",
    "        #   retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "        dfRow = pd.DataFrame([[edge[0], edge[1], (edge[2])['number_of_codonors'], res[0], res[1], res[2], res[3], res[4], res[5], instanceNum]], \n",
    "                           columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\", \"instanceNum\"])\n",
    "        #print(dfRow)\n",
    "        #print(dfRes.head(n=1))\n",
    "        #print(dfRes.tail(n=1))\n",
    "        dfRes = dfRes.append(dfRow, ignore_index=True)\n",
    "except:\n",
    "    dfRes.to_csv(savestr)#note - this is cumulative for now, not just the date I'm giving in the name\n",
    "    print('saved in except')\n",
    "    #I'd like to be able to print out the exception but the block stopped working when I did that.\n",
    "print('queries done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of ending names_to_results_dict: \n",
      "89\n",
      "saved no error\n"
     ]
    }
   ],
   "source": [
    "print(\"len of ending names_to_results_dict: \")\n",
    "print(len(names_to_results_dict))\n",
    "dfRes.to_csv(savestr)\n",
    "print('saved no error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
