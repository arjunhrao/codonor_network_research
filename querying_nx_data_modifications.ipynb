{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting program\n",
      "212\n",
      "22366\n",
      "1.0\n",
      "['MARIA CANTWELL', 'PATRICK KREITLOW', 'JANICE HAHN', 'RAFAEL CRUZ', 'ILEANA ROS-LEHTINEN', 'KEITH FITZGERALD', 'SUSAN THORN', 'JUDY BIGGERT', 'TAMMY BALDWIN', 'ALLEN WEST', 'MANAN TRIVEDI', 'MIKE COFFMAN', 'DAVID LOEBSACK', 'MIA LOVE', 'PAUL HIRSCHBIEL', 'DANIEL KILDEE', 'CLAIRE MCCASKILL', 'JOE GARCIA', 'TOBIAS SCHLINGENSIEPEN', 'STEVE ISRAEL', 'BRENDAN MULLEN', 'SHELLI YODER', 'SUZAN DELBENE', 'MAZIE HIRONO', 'MARK MURPHY', 'NICOLA TSONGAS', 'MATT VARILEK', 'DANIEL LUNGREN', 'NANCY PELOSI', 'DIANNE FEINSTEIN', 'JOHN DELANEY', 'TIM MURPHY', 'RANDOLPH ALTSCHULER', 'BETTY SUTTON', 'DANIEL MAFFEI', 'JOSEPH KYRILLOS', 'PETE GALLEGO', 'SPENCER BACHUS', 'RICHARD CARMONA', 'ROGER WILLIAMS', 'RICKY GILL', 'KIRSTEN GILLIBRAND', 'KAREN HARRINGTON', 'ALAN LOWENTHAL', 'JOHN BARROW', 'GERRY CONNOLLY', 'JULIA BROWNLEY', 'SHELDON WHITEHOUSE', 'PAUL TONKO', 'ROBERT KERREY', 'SALVATORE PACE', 'ROBERT CASEY', 'HEATHER WILSON', 'JOSEPH DONNELLY', 'BEN CHANDLER', 'ANN WAGNER', 'JAMES GRAVES', 'WILLIAM SOUTHERLAND', 'BARBARA LEE', 'VERNON BUCHANAN', 'ROBERT HURT', 'LOIS CAPPS', 'JULIAN SCHREIBMAN', 'PETER ROSKAM', 'JEFF DENHAM', 'PAM GULLESON', 'JOHN OCEGUERA', 'RAUL RUIZ', 'MIKE MCINTYRE', 'DEBBIE STABENOW', 'CHERI BUSTOS', 'DENNIS REHBERG', 'DEBBIE WASSERMAN SCHULTZ', 'NICOLE LEFAVOUR', 'JERRY MCNERNEY', 'SHELLEY BERKLEY', 'THOMAS PRICE', 'LOUISE SLAUGHTER', 'DEREK KILMER', 'ALAN GRAYSON', 'KEITH ELLISON', 'RICHARD NOLAN', 'TOM SMITH', 'CHRISTOPHER MURPHY', 'G. FOSTER', 'TODD AKIN', 'DONNA EDWARDS', 'ANGELA ZIMMANN', 'THOMAS COTTON', 'NITA LOWEY', 'FREDERICK UPTON', 'PATRICK MURPHY', 'JON TESTER', 'ANN BUERKLE', 'PATRICK MEEHAN', 'JAMES RENACCI', 'BRANDON SHAFFER', 'TOM LATHAM', 'SEAN MALONEY', 'DENNIS HECK', 'STEVEN HORSFORD', 'GARY PETERS', 'SCOTT BROWN', 'JOSE HERNANDEZ', 'TIMOTHY KAINE', 'JOE MIKLOSI', 'ANN KIRKPATRICK', 'MARK CRITZ', 'ANDREW HARRIS', 'ROB WALLACE', 'LINDA LINGLE', 'DAVID GILL', 'ROB ZERBAN', 'LLOYD DOGGETT', 'JOHN TIERNEY', 'RONALD BARBER', 'JEFFRY FLAKE', 'ERNEST POWELL', 'SHELLEY CAPITO', 'THOMAS CARPER', 'GARY MCDOWELL', 'TIMOTHY BISHOP', 'MICHAEL HONDA', 'DEBRA FISCHER', 'EDWARD ROYCE', 'DIANA DEGETTE', 'JESSE JACKSON', 'PAUL RYAN', 'CAROL SHEA-PORTER', 'TIMOTHY WALZ', 'KURT SCHRADER', 'BRADLEY SCHNEIDER', 'HEIDI HEITKAMP', 'JOHN GARAMENDI', 'ADAM HASNER', 'LOIS FRANKEL', 'ERIK PAULSEN', 'VALDEZ DEMINGS', 'KEVIN MCCARTHY', 'RUSH HOLT', 'STEVEN KING', 'PETER HOEKSTRA', 'ROBERT ANDREWS', 'AMERISH BERA', 'TOM RICE', 'JARED POLIS', 'RAUL GRIJALVA', 'ELIZABETH ESTY', 'DEAN HELLER', 'TOMMY THOMPSON', 'JAMES MCGOVERN', 'ANN KUSTER', 'L. DUCKWORTH', 'SUZANNE BONAMICI', 'JOHN DOUGLASS', 'KRISTI NOEM', 'BILL NELSON', 'JOYCE BEATTY', 'PETER DEFAZIO', 'CHRISTIE VILSACK', 'BERNARD SANDERS', 'CAROLYN MCCARTHY', 'JR BOUSTANY', 'PEDRO PIERLUISI', 'MARK TAKANO', 'JIM HIMES', 'AMY KLOBUCHAR', 'JO EMERSON', 'FRANCISCO CANSECO', 'REID RIBBLE', 'NAN HAYWORTH', 'RICHARD TISEI', 'WILLIAM ENYART', 'GEORGE ALLEN', 'RICHARD MOURDOCK', 'GRACE MENG', 'RONALD KIND', 'PATRICK TIBERI', 'MARK POCAN', 'RAYMOND CRAVAACK', 'HAKEEM JEFFRIES', 'MICHELE BACHMANN', 'KYRSTEN SINEMA', 'JUDY CHU', 'ERIC CANTOR', 'PATRICIA KEEVER', 'ELIZABETH WARREN', 'ANDREW RORABACK', 'SCOTT GARRETT', 'JOSEPH KENNEDY', 'BRUCE BRALEY', 'ROBERT DOLD', 'ROBERT MENENDEZ', 'BENJAMIN CARDIN', 'THOMAS HENSARLING', 'KATHLEEN HOCHUL', 'TIMOTHY SCOTT', 'WILLIAM OWENS', 'SCOTT PETERS', 'ANGUS KING', 'DAVID CICILLINE', 'GARLAND BARR', 'CONNIE MACK', 'SEAN DUFFY', 'SCOTT TIPTON', 'MARTIN HEINRICH', 'MARCY KAPTUR', 'STENY HOYER', 'NATHAN SHINAGAWA', 'JOE MANCHIN', 'WILLIAM PASCRELL', 'SHELLEY ADLER']\n"
     ]
    }
   ],
   "source": [
    "#read graph as edge list\n",
    "print('starting program')\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "#codonor graph - nodes are recipients. Edges are number of donors in common.\n",
    "H1 = nx.read_edgelist('co_donor_relabeled_nodes.txt',nodetype=str,delimiter='\\t!\\t')#data=(('number_of_codonors',int)))\n",
    "\n",
    "print(nx.number_of_nodes(H1) )#should be 212\n",
    "print( nx.number_of_edges(H1) )#should be 22366\n",
    "print(nx.density(H1) )\n",
    "\n",
    "print(H1.nodes()) #[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a list of the number of codonors\n",
    "num_codonors_list = []\n",
    "for e in H1.edges(data=True):\n",
    "    #e is a tuple\n",
    "    temp = (e[2])['number_of_codonors']\n",
    "    num_codonors_list.append(temp)\n",
    "#print(num_codonors_list)\n",
    "\n",
    "#get a list of the edges and sort by num codonors\n",
    "list_of_edges = []\n",
    "for e in H1.edges(data=True):\n",
    "    list_of_edges.append(e)\n",
    "list_of_edges = sorted(H1.edges(data=True), key=lambda tup: (tup[2]['number_of_codonors'],tup[1]) )#sort by number of codonors\n",
    "#should sort by the name of first node of edge one after this for consistency.\n",
    "\n",
    "\n",
    "#print(list_of_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_edges:\n",
      "22366\n",
      "1118.3\n",
      "high_bkt len:\n",
      "1118\n",
      "mid_bkt len:\n",
      "5591\n",
      "low_bkt len:\n",
      "15657\n"
     ]
    }
   ],
   "source": [
    "num_edges = len(list_of_edges)\n",
    "print(\"num_edges:\")\n",
    "print(num_edges)\n",
    "print(float(.05*num_edges))\n",
    "high_bkt = []#all edges above the 5% mark\n",
    "high_cutoff = num_edges - float(.05*num_edges)\n",
    "mid_bkt = []\n",
    "mid_cutoff = num_edges - float(.3*num_edges)\n",
    "low_bkt = []\n",
    "\n",
    "for e in range(num_edges):\n",
    "    if e >= high_cutoff:\n",
    "        high_bkt.append(list_of_edges[e])\n",
    "    elif e >= mid_cutoff:\n",
    "        mid_bkt.append(list_of_edges[e])\n",
    "    else:\n",
    "        low_bkt.append(list_of_edges[e])\n",
    "        \n",
    "#print(high_bkt)\n",
    "print(\"high_bkt len:\")\n",
    "print(len(high_bkt))\n",
    "print(\"mid_bkt len:\")\n",
    "print(len(mid_bkt))\n",
    "print(\"low_bkt len:\")\n",
    "print(len(low_bkt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm just going to do the entire high bucket as opposed to sampling from the lower ones because in the past the lower ones have led to poor results - at least in Google CSE, the values were often 0, which is undesirable. Doing the entire bucket as opposed to randomly sampling also allows me to not to overlap the edges I query on Google. I can always randomly sample from the bucket afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#get dictionary. Implement logic behind checking and saving within scraping function\n",
    "#https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "names_to_results_dict = {}\n",
    "with open('names_to_results_dict.pickle', 'rb') as handle:\n",
    "    names_to_results_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#host google.com 8.8.8.8\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#had to write a scraping method to work for me until I figure out why I can't find the server...\n",
    "def jaccard_google_search_scrape(search_term1, search_term2, api_key, cse_id, **kwargs):\n",
    "\n",
    "    num1 = -2\n",
    "    num2 = -2\n",
    "    #only scrape if it's not in the dict\n",
    "    if (names_to_results_dict.get(search_term1[1:-1]) is None):\n",
    "        r = requests.get(\"https://www.google.com/search\", params={'q':search_term1})\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "        if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "            num1 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "        else:\n",
    "            num1 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict[search_term1[1:-1]] = num1\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term1\")\n",
    "    else:\n",
    "        num1 = names_to_results_dict[search_term1[1:-1]]\n",
    "\n",
    "    if (names_to_results_dict.get(search_term2[1:-1]) is None):\n",
    "        r = requests.get(\"https://www.google.com/search\", params={'q':search_term2})\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "\n",
    "        if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "            num2 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "        else:\n",
    "            num2 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict[search_term2[1:-1]] = num2\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term2\")\n",
    "    else:\n",
    "        num2 = names_to_results_dict[search_term2[1:-1]]\n",
    "        \n",
    "    #the dict will keep updating as necessary during a set of queries, but must make sure to store before exiting\n",
    "    #in order to maintain the values we added. Since this exit could happen at any query, may as well save pickle every\n",
    "    #time we add to the dict.\n",
    "    \n",
    "    strBoth = search_term1 + ' ' + search_term2#I think this should be fine. There's so much noise anyways,\n",
    "    #and sometimes it increases the number of results, sometimes decreases, so using AND isn't better.\n",
    "    \n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':strBoth})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    numCommon = -2\n",
    "    if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "    else:\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "    \n",
    "    num1 = int(num1)\n",
    "    num2 = int(num2)\n",
    "    numCommon = int(numCommon)\n",
    "    #print('num results for str1:' + str(num1) )\n",
    "    #print('num results for str2:' + str(num2) )\n",
    "    #print('num results in common:' + str(numCommon) )\n",
    "    \n",
    "    \n",
    "    #sometimes denominator was 0. try except ZeroDivionError.\n",
    "    try:\n",
    "        jacInd = numCommon /(num1+num2-numCommon) #can still use / for regular division, but want decimals here.\n",
    "    except ZeroDivisionError:\n",
    "        jacInd = -1\n",
    "    \n",
    "    try:\n",
    "        divMin = numCommon / min(num1, num2)\n",
    "    except ZeroDivisionError:\n",
    "        divMin = -1\n",
    "    try:\n",
    "        divProd = numCommon / (num1*num2)\n",
    "    except:\n",
    "        divProd = -1\n",
    "    #retList is of format: num results for str1, num results for str2, num results in common, the Jaccard index,\n",
    "    #the intersection / the min, and the intersection / the product\n",
    "    retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    return retList\n",
    "\n",
    "print(\"method processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instanceNum: -1\n",
      "start index: 810\n",
      "end index: 900\n",
      "first edge to sample:\n",
      "('RICHARD CARMONA', 'ANN KUSTER', {'number_of_codonors': 2525})\n"
     ]
    }
   ],
   "source": [
    "#Figure out which edges to sample\n",
    "instanceNum = -1#same thing as like a machineNum\n",
    "print(\"instanceNum: \" + str(instanceNum))\n",
    "start = 810\n",
    "end = start+90\n",
    "print(\"start index: \" + str(start))\n",
    "print(\"end index: \" + str(end))\n",
    "#start = instanceNum*33\n",
    "#end = start + 33#we can technically do 33 edges safely\n",
    "#Note that if instanceNum were 1, we'd have [33:66].\n",
    "all_edges_to_sample = high_bkt[start:end]#partition based on machine number\n",
    "print(\"first edge to sample:\")\n",
    "print(all_edges_to_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "queries done\n"
     ]
    }
   ],
   "source": [
    "#space out requests more - want to write a cron script eventually for maybe ~6-8 requests per hour as opposed\n",
    "#to 100 per day which means 33 edges per day\n",
    "import time\n",
    "#time.sleep(1)\n",
    "\n",
    "\n",
    "#choose the first of the following two lines if it's the very start / origination of the file. Otherwise go with second\n",
    "#don't forget to save csv at the end\n",
    "#dfRes = pd.DataFrame(columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\"])\n",
    "dfRes = pd.read_csv('./data/codonor_network_queries_instance_0_5.21_highbkt_edges.csv',index_col=0)\n",
    "#^index_col: https://stackoverflow.com/questions/36519086/pandas-how-to-get-rid-of-unnamed-column-in-a-dataframe\n",
    "\n",
    "counter = start\n",
    "savestr = './data/codonor_network_queries_instance_0_5.21_highbkt_edges.csv'\n",
    "#'''\n",
    "try:\n",
    "    for edge in all_edges_to_sample:\n",
    "        time.sleep(1)\n",
    "        print(counter)\n",
    "        counter = counter + 1\n",
    "        name1 = '\"' + edge[0] + '\"'\n",
    "        name2 = '\"' + edge[1] + '\"'\n",
    "\n",
    "        my_api_key, my_cse_id = \"0\", \"0\"\n",
    "        res = jaccard_google_search_scrape(name1, name2, my_api_key, my_cse_id, num=1)\n",
    "        #   retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "        dfRow = pd.DataFrame([[edge[0], edge[1], (edge[2])['number_of_codonors'], res[0], res[1], res[2], res[3], res[4], res[5], instanceNum]], \n",
    "                           columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\", \"instanceNum\"])\n",
    "        #print(dfRow)\n",
    "        #print(dfRes.head(n=1))\n",
    "        #print(dfRes.tail(n=1))\n",
    "        dfRes = dfRes.append(dfRow, ignore_index=True)\n",
    "except:\n",
    "    dfRes.to_csv(savestr)#note - this is cumulative for now, not just the date I'm giving in the name\n",
    "    print('saved in except')\n",
    "    #I'd like to be able to print out the exception but the block stopped working when I did that.\n",
    "print('queries done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of ending names_to_results_dict: \n",
      "89\n",
      "saved no error\n"
     ]
    }
   ],
   "source": [
    "print(\"len of ending names_to_results_dict: \")\n",
    "print(len(names_to_results_dict))\n",
    "dfRes.to_csv(savestr)\n",
    "print('saved no error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
