{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting program\n",
      "212\n",
      "22366\n",
      "1.0\n",
      "['ROBERT ANDREWS', 'BERNARD SANDERS', 'GEORGE ALLEN', 'SPENCER BACHUS', 'EDWARD ROYCE']\n"
     ]
    }
   ],
   "source": [
    "#read graph as edge list\n",
    "print('starting program')\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "#codonor graph - nodes are recipients. Edges are number of donors in common.\n",
    "H1 = nx.read_edgelist('co_donor_relabeled_nodes.txt',nodetype=str,delimiter='\\t!\\t')#data=(('number_of_codonors',int)))\n",
    "\n",
    "print(nx.number_of_nodes(H1) )#should be 212\n",
    "print( nx.number_of_edges(H1) )#should be 22366\n",
    "print(nx.density(H1) )\n",
    "\n",
    "print(H1.nodes()[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a list of the number of codonors\n",
    "num_codonors_list = []\n",
    "for e in H1.edges(data=True):\n",
    "    #e is a tuple\n",
    "    temp = (e[2])['number_of_codonors']\n",
    "    num_codonors_list.append(temp)\n",
    "#print(num_codonors_list)\n",
    "\n",
    "#get a list of the edges and sort by num codonors\n",
    "list_of_edges = []\n",
    "for e in H1.edges(data=True):\n",
    "    list_of_edges.append(e)\n",
    "list_of_edges = sorted(H1.edges(data=True), key=lambda tup: (tup[2]['number_of_codonors'],tup[1]) )#sort by number of codonors\n",
    "#should sort by the name of first node of edge one after this for consistency.\n",
    "\n",
    "\n",
    "#print(list_of_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_edges:\n",
      "22366\n",
      "1118.3\n",
      "high_bkt len:\n",
      "1118\n",
      "mid_bkt len:\n",
      "5591\n",
      "low_bkt len:\n",
      "15657\n"
     ]
    }
   ],
   "source": [
    "num_edges = len(list_of_edges)\n",
    "print(\"num_edges:\")\n",
    "print(num_edges)\n",
    "print(float(.05*num_edges))\n",
    "high_bkt = []#all edges above the 5% mark\n",
    "high_cutoff = num_edges - float(.05*num_edges)\n",
    "mid_bkt = []\n",
    "mid_cutoff = num_edges - float(.3*num_edges)\n",
    "low_bkt = []\n",
    "\n",
    "for e in range(num_edges):\n",
    "    if e >= high_cutoff:\n",
    "        high_bkt.append(list_of_edges[e])\n",
    "    elif e >= mid_cutoff:\n",
    "        mid_bkt.append(list_of_edges[e])\n",
    "    else:\n",
    "        low_bkt.append(list_of_edges[e])\n",
    "        \n",
    "#print(high_bkt)\n",
    "print(\"high_bkt len:\")\n",
    "print(len(high_bkt))\n",
    "print(\"mid_bkt len:\")\n",
    "print(len(mid_bkt))\n",
    "print(\"low_bkt len:\")\n",
    "print(len(low_bkt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm just going to do the entire high bucket as opposed to sampling from the lower ones because in the past the lower ones have led to poor results - at least in Google CSE, the values were often 0, which is undesirable. Doing the entire bucket as opposed to randomly sampling also allows me to not to overlap the edges I query on Google. I can always randomly sample from the bucket afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#get dictionary. Implement logic behind checking and saving within scraping function\n",
    "#https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "names_to_results_dict = {}\n",
    "with open('names_to_results_dict.pickle', 'rb') as handle:\n",
    "    names_to_results_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#host google.com 8.8.8.8\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#had to write a scraping method to work for me until I figure out why I can't find the server...\n",
    "def jaccard_google_search_scrape(search_term1, search_term2, api_key, cse_id, **kwargs):\n",
    "\n",
    "    num1 = -2\n",
    "    num2 = -2\n",
    "    #only scrape if it's not in the dict\n",
    "    if (names_to_results_dict.get(search_term1[1:-1]) is None):\n",
    "        r = requests.get(\"https://www.google.com/search\", params={'q':search_term1})\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "        if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "            num1 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "        else:\n",
    "            num1 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict[search_term1[1:-1]] = num1\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term1\")\n",
    "    else:\n",
    "        num1 = names_to_results_dict[search_term1[1:-1]]\n",
    "\n",
    "    if (names_to_results_dict.get(search_term2[1:-1]) is None):\n",
    "        r = requests.get(\"https://www.google.com/search\", params={'q':search_term2})\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "\n",
    "        if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "            num2 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "        else:\n",
    "            num2 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict[search_term2[1:-1]] = num2\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term2\")\n",
    "    else:\n",
    "        num2 = names_to_results_dict[search_term2[1:-1]]\n",
    "        \n",
    "    #the dict will keep updating as necessary during a set of queries, but must make sure to store before exiting\n",
    "    #in order to maintain the values we added. Since this exit could happen at any query, may as well save pickle every\n",
    "    #time we add to the dict.\n",
    "    \n",
    "    strBoth = search_term1 + ' ' + search_term2#I think this should be fine. There's so much noise anyways,\n",
    "    #and sometimes it increases the number of results, sometimes decreases, so using AND isn't better.\n",
    "    \n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':strBoth})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    numCommon = -2\n",
    "    if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "    else:\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "    \n",
    "    num1 = int(num1)\n",
    "    num2 = int(num2)\n",
    "    numCommon = int(numCommon)\n",
    "    #print('num results for str1:' + str(num1) )\n",
    "    #print('num results for str2:' + str(num2) )\n",
    "    #print('num results in common:' + str(numCommon) )\n",
    "    \n",
    "    \n",
    "    #sometimes denominator was 0. try except ZeroDivionError.\n",
    "    try:\n",
    "        jacInd = numCommon /(num1+num2-numCommon) #can still use / for regular division, but want decimals here.\n",
    "    except ZeroDivisionError:\n",
    "        jacInd = -1\n",
    "    \n",
    "    try:\n",
    "        divMin = numCommon / min(num1, num2)\n",
    "    except ZeroDivisionError:\n",
    "        divMin = -1\n",
    "    try:\n",
    "        divProd = numCommon / (num1*num2)\n",
    "    except:\n",
    "        divProd = -1\n",
    "    #retList is of format: num results for str1, num results for str2, num results in common, the Jaccard index,\n",
    "    #the intersection / the min, and the intersection / the product\n",
    "    retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    return retList\n",
    "\n",
    "print(\"method processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instanceNum: 0\n",
      "start index: 91\n",
      "end index: 92\n",
      "first edge to sample:\n",
      "('L. DUCKWORTH', 'RICHARD NOLAN', {'number_of_codonors': 763})\n"
     ]
    }
   ],
   "source": [
    "#Figure out which edges to sample\n",
    "instanceNum = 0#same thing as like a machineNum\n",
    "print(\"instanceNum: \" + str(instanceNum))\n",
    "start = 91\n",
    "end = start+1\n",
    "print(\"start index: \" + str(start))\n",
    "print(\"end index: \" + str(end))\n",
    "#start = instanceNum*33\n",
    "#end = start + 33#we can technically do 33 edges safely\n",
    "#Note that if instanceNum were 1, we'd have [33:66].\n",
    "all_edges_to_sample = high_bkt[start:end]#partition based on machine number\n",
    "print(\"first edge to sample:\")\n",
    "print(all_edges_to_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "added to dict, saved term1\n",
      "added to dict, saved term2\n",
      "           str1           str2  num_codonors   num1    num2  numCommon  \\\n",
      "0  L. DUCKWORTH  RICHARD NOLAN           763  63900  227000          1   \n",
      "\n",
      "     jacInd    divMin       divProd  instanceNum  \n",
      "0  0.000003  0.000016  6.894032e-11            0  \n",
      "   Unnamed: 0    divMin       divProd  instanceNum   jacInd      num1  \\\n",
      "0         0.0  0.041003  9.090000e-08          3.0  0.01791  339000.0   \n",
      "\n",
      "       num2  numCommon  num_codonors            str1             str2  \n",
      "0  451000.0    13900.0         740.0  MARIA CANTWELL  ROBERT MENENDEZ  \n",
      "    Unnamed: 0  divMin  divProd  instanceNum  jacInd  num1  num2  numCommon  \\\n",
      "30         NaN     NaN      NaN          NaN     NaN   NaN   NaN        NaN   \n",
      "\n",
      "    num_codonors str1 str2  \n",
      "30           NaN  NaN  NaN  \n",
      "queries done\n"
     ]
    }
   ],
   "source": [
    "#space out requests more - want to write a cron script eventually for maybe ~6-8 requests per hour as opposed\n",
    "#to 100 per day which means 33 edges per day\n",
    "import time\n",
    "#time.sleep(1)\n",
    "\n",
    "\n",
    "#choose the first of the following two lines if it's the very start / origination of the file. Otherwise go with second\n",
    "#don't forget to save csv at the end\n",
    "#dfRes = pd.DataFrame(columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\"])\n",
    "dfRes = pd.read_csv('./data/codonor_network_queries_instance_0_5.20_highbkt_edges.csv')\n",
    "\n",
    "counter = start\n",
    "savestr = './data/codonor_network_queries_instance!_0_5.20_highbkt_edges.csv'\n",
    "#'''\n",
    "try:\n",
    "    for edge in all_edges_to_sample:\n",
    "        time.sleep(1)\n",
    "        print(counter)\n",
    "        counter = counter + 1\n",
    "        name1 = '\"' + edge[0] + '\"'\n",
    "        name2 = '\"' + edge[1] + '\"'\n",
    "\n",
    "        my_api_key, my_cse_id = \"0\", \"0\"\n",
    "        res = jaccard_google_search_scrape(name1, name2, my_api_key, my_cse_id, num=1)\n",
    "        #   retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "        dfRow = pd.DataFrame([[edge[0], edge[1], (edge[2])['number_of_codonors'], res[0], res[1], res[2], res[3], res[4], res[5], instanceNum]], \n",
    "                           columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\", \"instanceNum\"])\n",
    "        print(dfRow)\n",
    "        print(dfRes.head(n=1))\n",
    "        print(dfRes.tail(n=1))\n",
    "        dfRes = dfRes.append(dfRow, ignore_index=True)\n",
    "except:\n",
    "    dfRes.to_csv(savestr)\n",
    "    print('saved in except')\n",
    "print('queries done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending names_to_results_dict: \n",
      "{'ROBERT KERREY': 13000, 'LOUISE SLAUGHTER': 1530000, 'JAMES GRAVES': 202000, 'AMY KLOBUCHAR': 580000, 'RICHARD CARMONA': 182000, 'MARIA CANTWELL': 245000, 'L. DUCKWORTH': 63900, 'RICHARD NOLAN': 227000}\n",
      "saved no error\n"
     ]
    }
   ],
   "source": [
    "print(\"ending names_to_results_dict: \")\n",
    "print(names_to_results_dict)\n",
    "dfRes.to_csv(savestr)\n",
    "print('saved no error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of     Unnamed: 0    divMin       divProd  instanceNum    jacInd       num1  \\\n",
      "0          0.0  0.041003  9.090000e-08          3.0  0.017910   339000.0   \n",
      "1          1.0  0.005150  3.600000e-08          3.0  0.000539   143000.0   \n",
      "2          2.0  0.002326  1.700000e-08          3.0  0.000419    30100.0   \n",
      "3          3.0  0.054047  1.330000e-07          3.0  0.026978   383000.0   \n",
      "4          4.0  0.037430  5.280000e-08          3.0  0.012718   709000.0   \n",
      "5          5.0  0.072414  9.550000e-07          3.0  0.001093    75800.0   \n",
      "6          6.0  0.047619  1.120000e-06          3.0  0.019886    29400.0   \n",
      "7          7.0  0.032756  1.960000e-08          3.0  0.014353  1670000.0   \n",
      "8          8.0  0.037029  7.440000e-08          3.0  0.017913   498000.0   \n",
      "9          9.0  0.000558  5.210000e-09          3.0  0.000275   107000.0   \n",
      "10        10.0  0.018146  5.070000e-08          3.0  0.000735   358000.0   \n",
      "11        11.0  0.000224  3.170000e-10          3.0  0.000073   709000.0   \n",
      "12        12.0  0.000336  2.340000e-08          3.0  0.000129    14400.0   \n",
      "13        13.0  0.000321  4.530000e-10          3.0  0.000117   709000.0   \n",
      "14        14.0  0.002152  8.030000e-10          3.0  0.000086  2680000.0   \n",
      "15        15.0  0.455847  1.270000e-06          3.0  0.050158    41900.0   \n",
      "16        16.0  0.368132  9.460000e-07          3.0  0.075028   389000.0   \n",
      "17        17.0  0.002538  9.470000e-10          3.0  0.000129   143000.0   \n",
      "18        18.0  0.397959  2.410000e-06          3.0  0.064039    29400.0   \n",
      "19        19.0  0.241062  1.760000e-06          3.0  0.016340   137000.0   \n",
      "20        20.0  0.005736  1.330000e-08          3.0  0.000887    78800.0   \n",
      "21        21.0  0.010667  7.160000e-08          3.0  0.000796    12000.0   \n",
      "22        22.0  0.003611  9.430000e-09          3.0  0.000131   383000.0   \n",
      "23        23.0  0.065015  1.510000e-07          3.0  0.029666   431000.0   \n",
      "24        24.0  0.287912  8.040000e-07          3.0  0.061968   358000.0   \n",
      "25        25.0  0.000050  1.280000e-10          3.0  0.000005   389000.0   \n",
      "26        26.0  0.000455  7.110000e-09          3.0  0.000098    63900.0   \n",
      "27        27.0  0.010594  6.920000e-09          0.0  0.001237  1530000.0   \n",
      "28        28.0  0.002659  4.590000e-09          0.0  0.000636   580000.0   \n",
      "29        29.0  0.001787  7.290000e-09          0.0  0.000808   245000.0   \n",
      "30         NaN       NaN           NaN          NaN       NaN        NaN   \n",
      "31         NaN  0.000016  6.894032e-11          0.0  0.000003    63900.0   \n",
      "\n",
      "         num2  numCommon  num_codonors              str1               str2  \n",
      "0    451000.0    13900.0         740.0    MARIA CANTWELL    ROBERT MENENDEZ  \n",
      "1     16700.0       86.0         742.0     DONNA EDWARDS        JOE MIKLOSI  \n",
      "2    137000.0       70.0         742.0     JOHN OCEGUERA       MAZIE HIRONO  \n",
      "3    405000.0    20700.0         743.0         RAUL RUIZ   LOUISE SLAUGHTER  \n",
      "4    358000.0    13400.0         744.0         G. FOSTER    MARTIN HEINRICH  \n",
      "5      1160.0       84.0         745.0     RAUL GRIJALVA       AMERISH BERA  \n",
      "6     42400.0     1400.0         746.0        ANN KUSTER  BRADLEY SCHNEIDER  \n",
      "7   1270000.0    41600.0         746.0       BILL NELSON     PATRICK MURPHY  \n",
      "8    451000.0    16700.0         746.0   DEBBIE STABENOW    ROBERT MENENDEZ  \n",
      "9    104000.0       58.0         748.0        ROB ZERBAN       SEAN MALONEY  \n",
      "10    15100.0      274.0         748.0   MARTIN HEINRICH   CHRISTIE VILSACK  \n",
      "11   343000.0       77.0         749.0         G. FOSTER       ALAN GRAYSON  \n",
      "12     8920.0        3.0         750.0     ROBERT KERREY     WILLIAM ENYART  \n",
      "13   408000.0      131.0         751.0         G. FOSTER         JON TESTER  \n",
      "14   112000.0      241.0         752.0    JOSE HERNANDEZ       CHERI BUSTOS  \n",
      "15   358000.0    19100.0         752.0   BENJAMIN CARDIN   ELIZABETH WARREN  \n",
      "16    91000.0    33500.0         752.0    HEIDI HEITKAMP     JULIA BROWNLEY  \n",
      "17  2680000.0      363.0         753.0     DONNA EDWARDS     JOSE HERNANDEZ  \n",
      "18   165000.0    11700.0         753.0        ANN KUSTER     KYRSTEN SINEMA  \n",
      "19     9790.0     2360.0         754.0      MAZIE HIRONO      DANIEL MAFFEI  \n",
      "20   431000.0      452.0         755.0     RICHARD NOLAN         JOE GARCIA  \n",
      "21   149000.0      128.0         756.0      SHELLI YODER       LOIS FRANKEL  \n",
      "22    14400.0       52.0         756.0         RAUL RUIZ      ROBERT KERREY  \n",
      "23   343000.0    22300.0         756.0        JOE GARCIA       ALAN GRAYSON  \n",
      "24    91000.0    26200.0         756.0   MARTIN HEINRICH     JULIA BROWNLEY  \n",
      "25    40200.0        2.0         757.0    HEIDI HEITKAMP      RONALD BARBER  \n",
      "26    17600.0        8.0         757.0      L. DUCKWORTH      ERNEST POWELL  \n",
      "27   202000.0     2140.0         761.0  LOUISE SLAUGHTER       JAMES GRAVES  \n",
      "28   182000.0      484.0         761.0     AMY KLOBUCHAR    RICHARD CARMONA  \n",
      "29   202000.0      361.0         762.0    MARIA CANTWELL       JAMES GRAVES  \n",
      "30        NaN        NaN           NaN               NaN                NaN  \n",
      "31   227000.0        1.0         763.0      L. DUCKWORTH      RICHARD NOLAN  >\n"
     ]
    }
   ],
   "source": [
    "print(dfRes.head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
