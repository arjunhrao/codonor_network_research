{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting program\n",
      "212\n",
      "22366\n",
      "1.0\n",
      "['ROBERT ANDREWS', 'BERNARD SANDERS', 'GEORGE ALLEN', 'SPENCER BACHUS', 'EDWARD ROYCE']\n"
     ]
    }
   ],
   "source": [
    "#read graph as edge list\n",
    "print('starting program')\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "#codonor graph - nodes are recipients. Edges are number of donors in common.\n",
    "H1 = nx.read_edgelist('co_donor_relabeled_nodes.txt',nodetype=str,delimiter='\\t!\\t')#data=(('number_of_codonors',int)))\n",
    "\n",
    "print(nx.number_of_nodes(H1) )#should be 212\n",
    "print( nx.number_of_edges(H1) )#should be 22366\n",
    "print(nx.density(H1) )\n",
    "\n",
    "print(H1.nodes()[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a list of the number of codonors\n",
    "num_codonors_list = []\n",
    "for e in H1.edges(data=True):\n",
    "    #e is a tuple\n",
    "    temp = (e[2])['number_of_codonors']\n",
    "    num_codonors_list.append(temp)\n",
    "#print(num_codonors_list)\n",
    "\n",
    "#get a list of the edges and sort by num codonors\n",
    "list_of_edges = []\n",
    "for e in H1.edges(data=True):\n",
    "    list_of_edges.append(e)\n",
    "list_of_edges = sorted(H1.edges(data=True), key=lambda tup: tup[2]['number_of_codonors'])#sort by number of codonors\n",
    "#should sort by the name of first node of edge one after this for consistency.\n",
    "\n",
    "#print(list_of_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_edges:\n",
      "22366\n",
      "1118.3\n",
      "high_bkt len:\n",
      "1118\n",
      "mid_bkt len:\n",
      "5591\n",
      "low_bkt len:\n",
      "15657\n"
     ]
    }
   ],
   "source": [
    "num_edges = len(list_of_edges)\n",
    "print(\"num_edges:\")\n",
    "print(num_edges)\n",
    "print(float(.05*num_edges))\n",
    "high_bkt = []#all edges above the 5% mark\n",
    "high_cutoff = num_edges - float(.05*num_edges)\n",
    "mid_bkt = []\n",
    "mid_cutoff = num_edges - float(.3*num_edges)\n",
    "low_bkt = []\n",
    "\n",
    "for e in range(num_edges):\n",
    "    if e >= high_cutoff:\n",
    "        high_bkt.append(list_of_edges[e])\n",
    "    elif e >= mid_cutoff:\n",
    "        mid_bkt.append(list_of_edges[e])\n",
    "    else:\n",
    "        low_bkt.append(list_of_edges[e])\n",
    "        \n",
    "#print(high_bkt)\n",
    "print(\"high_bkt len:\")\n",
    "print(len(high_bkt))\n",
    "print(\"mid_bkt len:\")\n",
    "print(len(mid_bkt))\n",
    "print(\"low_bkt len:\")\n",
    "print(len(low_bkt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm just going to do the entire high bucket as opposed to sampling from the lower ones because in the past the lower ones have led to poor results - at least in Google CSE, the values were often 0, which is undesirable. Doing the entire bucket as opposed to randomly sampling also allows me to not to overlap the edges I query on Google. I can always randomly sample from the bucket afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#host google.com 8.8.8.8\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#had to write a scraping method to work for me until I figure out why I can't find the server...\n",
    "def jaccard_google_search_scrape(search_term1, search_term2, api_key, cse_id, **kwargs):\n",
    "\n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':search_term1})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    num1 = -2\n",
    "    if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "        num1 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "    else:\n",
    "        num1 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "\n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':search_term2})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    num2 = -2\n",
    "    if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "        num2 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "    else:\n",
    "        num2 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "    \n",
    "    strBoth = search_term1 + ' ' + search_term2#I think this should be fine. There's so much noise anyways,\n",
    "    #and sometimes it increases the number of results, sometimes decreases, so using AND isn't better.\n",
    "    \n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':strBoth})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    numCommon = -2\n",
    "    if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "    else:\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "    \n",
    "    num1 = int(num1)\n",
    "    num2 = int(num2)\n",
    "    numCommon = int(numCommon)\n",
    "    #print('num results for str1:' + str(num1) )\n",
    "    #print('num results for str2:' + str(num2) )\n",
    "    #print('num results in common:' + str(numCommon) )\n",
    "    \n",
    "    \n",
    "    #sometimes denominator was 0. try except ZeroDivionError.\n",
    "    try:\n",
    "        jacInd = numCommon /(num1+num2-numCommon) #can still use / for regular division, but want decimals here.\n",
    "    except ZeroDivisionError:\n",
    "        jacInd = -1\n",
    "    \n",
    "    try:\n",
    "        divMin = numCommon / min(num1, num2)\n",
    "    except ZeroDivisionError:\n",
    "        divMin = -1\n",
    "    try:\n",
    "        divProd = numCommon / (num1*num2)\n",
    "    except:\n",
    "        divProd = -1\n",
    "    #retList is of format: num results for str1, num results for str2, num results in common, the Jaccard index,\n",
    "    #the intersection / the min, and the intersection / the product\n",
    "    retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    return retList\n",
    "\n",
    "print(\"method processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first edge to sample:\n",
      "('LOUISE SLAUGHTER', 'JAMES GRAVES', {'number_of_codonors': 761})\n"
     ]
    }
   ],
   "source": [
    "#Figure out which edges to sample\n",
    "instanceNum = 2#same thing as like a machineNum\n",
    "start = 88\n",
    "end = start+42\n",
    "#start = instanceNum*33\n",
    "#end = start + 33#we can technically do 33 edges safely\n",
    "#Note that if instanceNum were 1, we'd have [33:66].\n",
    "all_edges_to_sample = high_bkt[start:end]#partition based on machine number\n",
    "print(\"first edge to sample:\")\n",
    "print(all_edges_to_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#get dictionary. Implement logic behind checking and saving within for loop \n",
    "#https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a40af1b5cc4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmy_api_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_cse_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaccard_google_search_scrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_api_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_cse_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#   retList = [num1, num2, numCommon, jacInd, divMin, divProd]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     dfRow = pd.DataFrame([[edge[0], edge[1], (edge[2])['number_of_codonors'], res[0], res[1], res[2], res[3], res[4], res[5]]], \n",
      "\u001b[0;32m<ipython-input-5-3342f921e409>\u001b[0m in \u001b[0;36mjaccard_google_search_scrape\u001b[0;34m(search_term1, search_term2, api_key, cse_id, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"resultStats\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnum2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"About\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mnum2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#space out requests more - want to write a cron script eventually for maybe ~6-8 requests per hour as opposed\n",
    "#to 100 per day which means 33 edges per day\n",
    "import time\n",
    "#time.sleep(1)\n",
    "\n",
    "\n",
    "#choose the first of the following two lines if it's the very start / origination of the file. Otherwise go with second\n",
    "#don't forget to save csv at the end\n",
    "#dfRes = pd.DataFrame(columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\"])\n",
    "dfRes = pd.read_csv('./co_donor_google_queries_fin_6.csv')\n",
    "\n",
    "counter = start\n",
    "savestr = './data/codonor_network_queries_instance_' + str(instanceNum) + '_5.20_highbkt_edges_' + str(start) + '_to_' + str(end-1) + '.csv'\n",
    "#'''\n",
    "try:\n",
    "    for edge in all_edges_to_sample:\n",
    "        time.sleep(1)\n",
    "        print(counter)\n",
    "        counter = counter + 1\n",
    "        name1 = '\"' + edge[0] + '\"'\n",
    "        name2 = '\"' + edge[1] + '\"'\n",
    "\n",
    "        my_api_key, my_cse_id = \"0\", \"0\"\n",
    "        res = jaccard_google_search_scrape(name1, name2, my_api_key, my_cse_id, num=1)\n",
    "        #   retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "        dfRow = pd.DataFrame([[edge[0], edge[1], (edge[2])['number_of_codonors'], res[0], res[1], res[2], res[3], res[4], res[5]]], \n",
    "                           columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\"])\n",
    "\n",
    "        dfRes = dfRes.append(dfRow, ignore_index=True)\n",
    "except:\n",
    "    dfRes.to_csv(savestr)\n",
    "    print('saved in except')\n",
    "print('queries done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "dfRes.to_csv(savestr)\n",
    "print('saved no error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
