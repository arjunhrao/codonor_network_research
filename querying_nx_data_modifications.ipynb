{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting program\n",
      "212\n",
      "22366\n",
      "1.0\n",
      "['ROBERT ANDREWS', 'BERNARD SANDERS', 'GEORGE ALLEN', 'SPENCER BACHUS', 'EDWARD ROYCE']\n"
     ]
    }
   ],
   "source": [
    "#read graph as edge list\n",
    "print('starting program')\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "#codonor graph - nodes are recipients. Edges are number of donors in common.\n",
    "H1 = nx.read_edgelist('co_donor_relabeled_nodes.txt',nodetype=str,delimiter='\\t!\\t')#data=(('number_of_codonors',int)))\n",
    "\n",
    "print(nx.number_of_nodes(H1) )#should be 212\n",
    "print( nx.number_of_edges(H1) )#should be 22366\n",
    "print(nx.density(H1) )\n",
    "\n",
    "print(H1.nodes()) #[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a list of the number of codonors\n",
    "num_codonors_list = []\n",
    "for e in H1.edges(data=True):\n",
    "    #e is a tuple\n",
    "    temp = (e[2])['number_of_codonors']\n",
    "    num_codonors_list.append(temp)\n",
    "#print(num_codonors_list)\n",
    "\n",
    "#get a list of the edges and sort by num codonors\n",
    "list_of_edges = []\n",
    "for e in H1.edges(data=True):\n",
    "    list_of_edges.append(e)\n",
    "list_of_edges = sorted(H1.edges(data=True), key=lambda tup: (tup[2]['number_of_codonors'],tup[1]) )#sort by number of codonors\n",
    "#should sort by the name of first node of edge one after this for consistency.\n",
    "\n",
    "\n",
    "#print(list_of_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_edges:\n",
      "22366\n",
      "1118.3\n",
      "high_bkt len:\n",
      "1118\n",
      "mid_bkt len:\n",
      "5591\n",
      "low_bkt len:\n",
      "15657\n"
     ]
    }
   ],
   "source": [
    "num_edges = len(list_of_edges)\n",
    "print(\"num_edges:\")\n",
    "print(num_edges)\n",
    "print(float(.05*num_edges))\n",
    "high_bkt = []#all edges above the 5% mark\n",
    "high_cutoff = num_edges - float(.05*num_edges)\n",
    "mid_bkt = []\n",
    "mid_cutoff = num_edges - float(.3*num_edges)\n",
    "low_bkt = []\n",
    "\n",
    "for e in range(num_edges):\n",
    "    if e >= high_cutoff:\n",
    "        high_bkt.append(list_of_edges[e])\n",
    "    elif e >= mid_cutoff:\n",
    "        mid_bkt.append(list_of_edges[e])\n",
    "    else:\n",
    "        low_bkt.append(list_of_edges[e])\n",
    "        \n",
    "#print(high_bkt)\n",
    "print(\"high_bkt len:\")\n",
    "print(len(high_bkt))\n",
    "print(\"mid_bkt len:\")\n",
    "print(len(mid_bkt))\n",
    "print(\"low_bkt len:\")\n",
    "print(len(low_bkt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm just going to do the entire high bucket as opposed to sampling from the lower ones because in the past the lower ones have led to poor results - at least in Google CSE, the values were often 0, which is undesirable. Doing the entire bucket as opposed to randomly sampling also allows me to not to overlap the edges I query on Google. I can always randomly sample from the bucket afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#get dictionary. Implement logic behind checking and saving within scraping function\n",
    "#https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "names_to_results_dict = {}\n",
    "with open('names_to_results_dict.pickle', 'rb') as handle:\n",
    "    names_to_results_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#host google.com 8.8.8.8\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#had to write a scraping method to work for me until I figure out why I can't find the server...\n",
    "def jaccard_google_search_scrape(search_term1, search_term2, api_key, cse_id, **kwargs):\n",
    "\n",
    "    num1 = -2\n",
    "    num2 = -2\n",
    "    #only scrape if it's not in the dict\n",
    "    if (names_to_results_dict.get(search_term1[1:-1]) is None):\n",
    "        r = requests.get(\"https://www.google.com/search\", params={'q':search_term1})\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "        if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "            num1 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "        else:\n",
    "            num1 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict[search_term1[1:-1]] = num1\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term1\")\n",
    "    else:\n",
    "        num1 = names_to_results_dict[search_term1[1:-1]]\n",
    "\n",
    "    if (names_to_results_dict.get(search_term2[1:-1]) is None):\n",
    "        r = requests.get(\"https://www.google.com/search\", params={'q':search_term2})\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "\n",
    "        if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "            num2 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "        else:\n",
    "            num2 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict[search_term2[1:-1]] = num2\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term2\")\n",
    "    else:\n",
    "        num2 = names_to_results_dict[search_term2[1:-1]]\n",
    "        \n",
    "    #the dict will keep updating as necessary during a set of queries, but must make sure to store before exiting\n",
    "    #in order to maintain the values we added. Since this exit could happen at any query, may as well save pickle every\n",
    "    #time we add to the dict.\n",
    "    \n",
    "    strBoth = search_term1 + ' ' + search_term2#I think this should be fine. There's so much noise anyways,\n",
    "    #and sometimes it increases the number of results, sometimes decreases, so using AND isn't better.\n",
    "    \n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':strBoth})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    numCommon = -2\n",
    "    if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "    else:\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "    \n",
    "    num1 = int(num1)\n",
    "    num2 = int(num2)\n",
    "    numCommon = int(numCommon)\n",
    "    #print('num results for str1:' + str(num1) )\n",
    "    #print('num results for str2:' + str(num2) )\n",
    "    #print('num results in common:' + str(numCommon) )\n",
    "    \n",
    "    \n",
    "    #sometimes denominator was 0. try except ZeroDivionError.\n",
    "    try:\n",
    "        jacInd = numCommon /(num1+num2-numCommon) #can still use / for regular division, but want decimals here.\n",
    "    except ZeroDivisionError:\n",
    "        jacInd = -1\n",
    "    \n",
    "    try:\n",
    "        divMin = numCommon / min(num1, num2)\n",
    "    except ZeroDivisionError:\n",
    "        divMin = -1\n",
    "    try:\n",
    "        divProd = numCommon / (num1*num2)\n",
    "    except:\n",
    "        divProd = -1\n",
    "    #retList is of format: num results for str1, num results for str2, num results in common, the Jaccard index,\n",
    "    #the intersection / the min, and the intersection / the product\n",
    "    retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    return retList\n",
    "\n",
    "print(\"method processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instanceNum: 0\n",
      "start index: 92\n",
      "end index: 120\n",
      "first edge to sample:\n",
      "('ANN KIRKPATRICK', 'JULIA BROWNLEY', {'number_of_codonors': 764})\n"
     ]
    }
   ],
   "source": [
    "#Figure out which edges to sample\n",
    "instanceNum = 2#same thing as like a machineNum\n",
    "print(\"instanceNum: \" + str(instanceNum))\n",
    "start = 160\n",
    "end = start+40\n",
    "print(\"start index: \" + str(start))\n",
    "print(\"end index: \" + str(end))\n",
    "#start = instanceNum*33\n",
    "#end = start + 33#we can technically do 33 edges safely\n",
    "#Note that if instanceNum were 1, we'd have [33:66].\n",
    "all_edges_to_sample = high_bkt[start:end]#partition based on machine number\n",
    "print(\"first edge to sample:\")\n",
    "print(all_edges_to_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "added to dict, saved term1\n",
      "added to dict, saved term2\n",
      "93\n",
      "added to dict, saved term1\n",
      "94\n",
      "added to dict, saved term2\n",
      "95\n",
      "added to dict, saved term2\n",
      "96\n",
      "added to dict, saved term1\n",
      "added to dict, saved term2\n",
      "97\n",
      "added to dict, saved term2\n",
      "98\n",
      "added to dict, saved term1\n",
      "added to dict, saved term2\n",
      "99\n",
      "added to dict, saved term1\n",
      "added to dict, saved term2\n",
      "100\n",
      "added to dict, saved term1\n",
      "added to dict, saved term2\n",
      "101\n",
      "102\n",
      "added to dict, saved term1\n",
      "103\n",
      "added to dict, saved term1\n",
      "104\n",
      "added to dict, saved term2\n",
      "105\n",
      "added to dict, saved term1\n",
      "106\n",
      "added to dict, saved term1\n",
      "107\n",
      "added to dict, saved term1\n",
      "added to dict, saved term2\n",
      "108\n",
      "added to dict, saved term1\n",
      "added to dict, saved term2\n",
      "109\n",
      "added to dict, saved term1\n",
      "added to dict, saved term2\n",
      "110\n",
      "added to dict, saved term1\n",
      "111\n",
      "112\n",
      "added to dict, saved term2\n",
      "113\n",
      "added to dict, saved term1\n",
      "114\n",
      "added to dict, saved term1\n",
      "115\n",
      "added to dict, saved term2\n",
      "116\n",
      "added to dict, saved term1\n",
      "117\n",
      "added to dict, saved term2\n",
      "118\n",
      "119\n",
      "queries done\n"
     ]
    }
   ],
   "source": [
    "#space out requests more - want to write a cron script eventually for maybe ~6-8 requests per hour as opposed\n",
    "#to 100 per day which means 33 edges per day\n",
    "import time\n",
    "#time.sleep(1)\n",
    "\n",
    "\n",
    "#choose the first of the following two lines if it's the very start / origination of the file. Otherwise go with second\n",
    "#don't forget to save csv at the end\n",
    "#dfRes = pd.DataFrame(columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\"])\n",
    "dfRes = pd.read_csv('./data/codonor_network_queries_instance_0_5.21_highbkt_edges.csv',index_col=0)\n",
    "#^index_col: https://stackoverflow.com/questions/36519086/pandas-how-to-get-rid-of-unnamed-column-in-a-dataframe\n",
    "\n",
    "counter = start\n",
    "savestr = './data/codonor_network_queries_instance_0_5.21_highbkt_edges.csv'\n",
    "#'''\n",
    "try:\n",
    "    for edge in all_edges_to_sample:\n",
    "        time.sleep(1)\n",
    "        print(counter)\n",
    "        counter = counter + 1\n",
    "        name1 = '\"' + edge[0] + '\"'\n",
    "        name2 = '\"' + edge[1] + '\"'\n",
    "\n",
    "        my_api_key, my_cse_id = \"0\", \"0\"\n",
    "        res = jaccard_google_search_scrape(name1, name2, my_api_key, my_cse_id, num=1)\n",
    "        #   retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "        dfRow = pd.DataFrame([[edge[0], edge[1], (edge[2])['number_of_codonors'], res[0], res[1], res[2], res[3], res[4], res[5], instanceNum]], \n",
    "                           columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\", \"instanceNum\"])\n",
    "        #print(dfRow)\n",
    "        #print(dfRes.head(n=1))\n",
    "        #print(dfRes.tail(n=1))\n",
    "        dfRes = dfRes.append(dfRow, ignore_index=True)\n",
    "except:\n",
    "    dfRes.to_csv(savestr)#note - this is cumulative for now, not just the date I'm giving in the name\n",
    "    print('saved in except')\n",
    "print('queries done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of ending names_to_results_dict: \n",
      "40\n",
      "saved no error\n"
     ]
    }
   ],
   "source": [
    "print(\"len of ending names_to_results_dict: \")\n",
    "print(len(names_to_results_dict))\n",
    "dfRes.to_csv(savestr)\n",
    "print('saved no error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
