{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting program\n",
      "528\n",
      "34980\n",
      "0.2514231499051233\n"
     ]
    }
   ],
   "source": [
    "#read graph as edge list\n",
    "print('starting program')\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "#codonor graph - nodes are recipients. Edges are number of donors in common.\n",
    "H1 = nx.read_edgelist('co_donor_relabeled_nodes.txt',nodetype=str,delimiter='\\t!\\t')#data=(('number_of_codonors',int)))\n",
    "\n",
    "print(nx.number_of_nodes(H1) )#should be 212\n",
    "print( nx.number_of_edges(H1) )#should be 22366\n",
    "print(nx.density(H1) )\n",
    "\n",
    "#print(H1.nodes()) #[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of the number of codonors\n",
    "num_codonors_list = []\n",
    "for e in H1.edges(data=True):\n",
    "    #e is a tuple\n",
    "    temp = (e[2])['number_of_codonors']\n",
    "    num_codonors_list.append(temp)\n",
    "#print(num_codonors_list)\n",
    "\n",
    "#get a list of the edges and sort by num codonors\n",
    "list_of_edges = []\n",
    "#for e in H1.edges(data=True):\n",
    "#    list_of_edges.append(e)\n",
    "list_of_edges = sorted(H1.edges(data=True), key=lambda tup: (tup[2]['number_of_codonors'],tup[1]) )#sort by number of codonors\n",
    "#should sort by the name of first node of edge one after this for consistency.\n",
    "\n",
    "\n",
    "#print(list_of_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_edges:\n",
      "34980\n",
      "1749.0\n",
      "high_bkt len:\n",
      "1749\n",
      "mid_bkt len:\n",
      "8745\n",
      "low_bkt len:\n",
      "24486\n"
     ]
    }
   ],
   "source": [
    "num_edges = len(list_of_edges)\n",
    "print(\"num_edges:\")\n",
    "print(num_edges)\n",
    "print(float(.05*num_edges))\n",
    "high_bkt = []#all edges above the 5% mark\n",
    "high_cutoff = num_edges - float(.05*num_edges)\n",
    "mid_bkt = []\n",
    "mid_cutoff = num_edges - float(.3*num_edges)\n",
    "low_bkt = []\n",
    "\n",
    "for e in range(num_edges):\n",
    "    if e >= high_cutoff:\n",
    "        high_bkt.append(list_of_edges[e])\n",
    "    elif e >= mid_cutoff:\n",
    "        mid_bkt.append(list_of_edges[e])\n",
    "    else:\n",
    "        low_bkt.append(list_of_edges[e])\n",
    "        \n",
    "#print(high_bkt)\n",
    "print(\"high_bkt len:\")\n",
    "print(len(high_bkt))\n",
    "print(\"mid_bkt len:\")\n",
    "print(len(mid_bkt))\n",
    "print(\"low_bkt len:\")\n",
    "print(len(low_bkt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm just going to do the entire high bucket as opposed to sampling from the lower ones because in the past the lower ones have led to poor results - at least in Google CSE, the values were often 0, which is undesirable. Doing the entire bucket as opposed to randomly sampling also allows me to not to overlap the edges I query on Google. I can always randomly sample from the bucket afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#get dictionary. Implement logic behind checking and saving within scraping function\n",
    "#https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "names_to_results_dict_CSE = {}\n",
    "with open('names_to_results_dict_CSE.pickle', 'rb') as handle:\n",
    "    names_to_results_dict_CSE = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=5, releaselevel='final', serial=0)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#code for methods for performing the google queries- see JaccardQueriesV2.ipynb\n",
    "#did ulimit -n 2048 in terminal (actually 512)\n",
    "#see https://stackoverflow.com/questions/39537731/errno-24-too-many-open-files-but-i-am-not-opening-files\n",
    "#OSError: [Errno 24] Too many open files: '/Users/ravirao/anaconda/lib/python3.5/site-packages/googleapiclient/__init__.py'\n",
    "#Turns out the error was on their side, their server\n",
    "\n",
    "#V2 returns a list, and has a method to print out the contents of the list.\n",
    "#V3 (or maybe i'll do it in this doc) will be the one I will prob use overall - read a csv file,\n",
    "#randomly decide which ppl to take,\n",
    "#check if they've already been done before querying each of the 3 possibilities, etc.\n",
    "\n",
    "#this link was very helpful:\n",
    "#https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "#I was having trouble creating the custom search engine in the first place, as detailed more in my notes\n",
    "#but I got it, and an API key, and now I think I can just use the API in python\n",
    "import sys\n",
    "sys.version\n",
    "print(sys.version_info)\n",
    "#this is python 3.5, so why can't I access the googleapiclient module? I ran the command:\n",
    "#pip install --upgrade google-api-python-client\n",
    "#so it should work\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "import pprint\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "#Search engine ID for the custom search engine I created (Political Candidates Jaccard Index Queries):\n",
    "#017540791571327827296:xy6h11dupmm\n",
    "\n",
    "#The API key is different; I thought it would have to be connected with my custom search engine upon generation but it wasn't\n",
    "#Oh, I have to use my search engine ID as input as well.\n",
    "\n",
    "#arjunhrao1\n",
    "#my_api_key = \"AIzaSyDNjEEUVZNXM_EcKoYmITcuoiZEEq6hQUs\"\n",
    "#my_cse_id = \"005060056456624332840:lxtmrbo6eho\"\n",
    "#other - from co-recip\n",
    "#my_api_key = \"AIzaSyCNO5wrFa5RcehkMyY6rxrkjlHVFaJ4rF4\"\n",
    "#my_cse_id = \"012951132720784451195:aybfyk7h8wm\"\n",
    "#waterfrogs\n",
    "#my_api_key = \"AIzaSyDNjEEUVZNXM_EcKoYmITcuoiZEEq6hQUs\"\n",
    "#my_cse_id = \"005060056456624332840:lxtmrbo6eho\"\n",
    "#arjunhkrao\n",
    "my_api_key = \"AIzaSyDAMPHn_MDJqbnaN29h67-rIeEC1R2jOaM\"\n",
    "my_cse_id = \"017540791571327827296:xy6h11dupmm\"\n",
    "\n",
    "\n",
    "#These three methods will assume you've already put quotes around the search terms you pass in\n",
    "\n",
    "def basic_google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()#res is of type dict\n",
    "    #^can have more arguments, e.g. number of results to store\n",
    "    #print(type(res))\n",
    "    return res['queries']\n",
    "    #return res['queries']['request'][0]['totalResults'] #note that the number is of type str\n",
    "    #^commented out bc want more info available potentially - can be more specific for jaccard\n",
    "    \n",
    "    #https://developers.google.com/custom-search/json-api/v1/reference/cse/list #how list method works\n",
    "    \n",
    "def exact_google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, exactTerms=search_term, cx=cse_id, **kwargs).execute()#res is of type dict\n",
    "    #^can have more arguments, e.g. number of results to store\n",
    "    #print(type(res))\n",
    "    return res['queries']\n",
    "\n",
    "\n",
    "def jaccard_google_search(search_term1, search_term2, api_key, cse_id, **kwargs):\n",
    "    \n",
    "    num1 = -2\n",
    "    num2 = -2\n",
    "    if (names_to_results_dict_CSE.get(search_term1[1:-1]) is None):\n",
    "        results = basic_google_search(search_term1, api_key, cse_id, **kwargs)\n",
    "        num1 = results['request'][0]['totalResults'] #note that the number is of type str  \n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict_CSE[search_term1[1:-1]] = int(num1)\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict_CSE.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict_CSE, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term1\")\n",
    "    else:\n",
    "        num1 = names_to_results_dict_CSE[search_term1[1:-1]]\n",
    "        \n",
    "    if (names_to_results_dict_CSE.get(search_term2[1:-1]) is None):\n",
    "        results = basic_google_search(search_term2, api_key, cse_id, **kwargs)\n",
    "        num2 = results['request'][0]['totalResults'] #note that the number is of type str\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict_CSE[search_term2[1:-1]] = int(num2)\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict_CSE.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_CSE_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term1\")\n",
    "    else:\n",
    "        num2 = names_to_results_dict_CSE[search_term2[1:-1]]\n",
    "    \n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    strBoth = search_term1 + ' ' + search_term2\n",
    "    #how do you put multiple phrases in as exactTerms? idk if that's possible... #outdated comment\n",
    "    results = basic_google_search(strBoth, api_key, cse_id, **kwargs)\n",
    "    numCommon = results['request'][0]['totalResults'] #note that the number is of type str\n",
    "    \n",
    "    num1 = int(num1)\n",
    "    num2 = int(num2)\n",
    "    numCommon = int(numCommon)\n",
    "    #print('num results for str1:' + str(num1) )\n",
    "    #print('num results for str2:' + str(num2) )\n",
    "    #print('num results in common:' + str(numCommon) )\n",
    "    \n",
    "    #sometimes denominator was 0. try except ZeroDivionError.\n",
    "    try:\n",
    "        jacInd = numCommon /(num1+num2-numCommon) #can still use / for regular division, but want decimals here.\n",
    "    except ZeroDivisionError:\n",
    "        jacInd = -1\n",
    "    \n",
    "    try:\n",
    "        divMin = numCommon / min(num1, num2)\n",
    "    except ZeroDivisionError:\n",
    "        divMin = -1\n",
    "    try:\n",
    "        divProd = numCommon / (num1*num2)\n",
    "    except:\n",
    "        divProd = -1\n",
    "    #retList is of format: num results for str1, num results for str2, num results in common, the Jaccard index,\n",
    "    #the intersection / the min, and the intersection / the product\n",
    "    retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    return retList\n",
    "\n",
    "print('done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#host google.com 8.8.8.8\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#had to write a scraping method to work for me until I figure out why I can't find the server...\n",
    "def jaccard_google_search_scrape(search_term1, search_term2, api_key, cse_id, **kwargs):\n",
    "\n",
    "    num1 = -2\n",
    "    num2 = -2\n",
    "    #only scrape if it's not in the dict\n",
    "    if (names_to_results_dict.get(search_term1[1:-1]) is None):\n",
    "        r = requests.get(\"https://www.google.com/search\", params={'q':search_term1})\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "        if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "            num1 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "        else:\n",
    "            num1 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict[search_term1[1:-1]] = num1\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term1\")\n",
    "    else:\n",
    "        num1 = names_to_results_dict[search_term1[1:-1]]\n",
    "\n",
    "    if (names_to_results_dict.get(search_term2[1:-1]) is None):\n",
    "        r = requests.get(\"https://www.google.com/search\", params={'q':search_term2})\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "\n",
    "        if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "            num2 = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "        else:\n",
    "            num2 = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "        #save this resulting value for the search term to the dict\n",
    "        names_to_results_dict[search_term2[1:-1]] = num2\n",
    "        #save pickle\n",
    "        with open('names_to_results_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(names_to_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"added to dict, saved term2\")\n",
    "    else:\n",
    "        num2 = names_to_results_dict[search_term2[1:-1]]\n",
    "        \n",
    "    #the dict will keep updating as necessary during a set of queries, but must make sure to store before exiting\n",
    "    #in order to maintain the values we added. Since this exit could happen at any query, may as well save pickle every\n",
    "    #time we add to the dict.\n",
    "    \n",
    "    strBoth = search_term1 + ' ' + search_term2#I think this should be fine. There's so much noise anyways,\n",
    "    #and sometimes it increases the number of results, sometimes decreases, so using AND isn't better.\n",
    "    \n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':strBoth})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    numCommon = -2\n",
    "    if (res.text.replace(\",\", \"\").split()[0].strip() == \"About\"):\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[1].strip() )\n",
    "    else:\n",
    "        numCommon = int(res.text.replace(\",\", \"\").split()[0].strip() )\n",
    "    \n",
    "    num1 = int(num1)\n",
    "    num2 = int(num2)\n",
    "    numCommon = int(numCommon)\n",
    "    #print('num results for str1:' + str(num1) )\n",
    "    #print('num results for str2:' + str(num2) )\n",
    "    #print('num results in common:' + str(numCommon) )\n",
    "    \n",
    "    \n",
    "    #sometimes denominator was 0. try except ZeroDivionError.\n",
    "    try:\n",
    "        jacInd = numCommon /(num1+num2-numCommon) #can still use / for regular division, but want decimals here.\n",
    "    except ZeroDivisionError:\n",
    "        jacInd = -1\n",
    "    \n",
    "    try:\n",
    "        divMin = numCommon / min(num1, num2)\n",
    "    except ZeroDivisionError:\n",
    "        divMin = -1\n",
    "    try:\n",
    "        divProd = numCommon / (num1*num2)\n",
    "    except:\n",
    "        divProd = -1\n",
    "    #retList is of format: num results for str1, num results for str2, num results in common, the Jaccard index,\n",
    "    #the intersection / the min, and the intersection / the product\n",
    "    retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    return retList\n",
    "\n",
    "print(\"method processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instanceNum: 0\n",
      "start index: 0\n",
      "end index: 100\n",
      "first edge to sample:\n",
      "(' GEORGE RIVELL', 'LAURA MINER', {'number_of_codonors': 44})\n"
     ]
    }
   ],
   "source": [
    "#Figure out which edges to sample\n",
    "instanceNum = 0#same thing as like a machineNum\n",
    "print(\"instanceNum: \" + str(instanceNum))\n",
    "start = 0\n",
    "end = start + 100 #just go to the end\n",
    "print(\"start index: \" + str(start))\n",
    "print(\"end index: \" + str(end))\n",
    "#start = instanceNum*33\n",
    "#end = start + 33#we can technically do 33 edges safely\n",
    "#Note that if instanceNum were 1, we'd have [33:66].\n",
    "all_edges_to_sample = high_bkt[start:end]#partition based on machine number\n",
    "#all_edges_to_sample.append(mid_bkt[-80:])\n",
    "print(\"first edge to sample:\")\n",
    "print(all_edges_to_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 400 when requesting https://www.googleapis.com/customsearch/v1?q=%22+GEORGE+RIVELL%22&cx=0&num=1&key=0&alt=json returned \"Bad Request\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-8eace55d9ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#*********************v regular or scrape****************************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaccard_google_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_api_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_cse_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m#   retList = [num1, num2, numCommon, jacInd, divMin, divProd]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     dfRow = pd.DataFrame([[edge[0], edge[1], (edge[2])['number_of_codonors'], res[0], res[1], res[2], res[3], res[4], res[5], instanceNum]], \n",
      "\u001b[0;32m<ipython-input-139-1e43d9920f5b>\u001b[0m in \u001b[0;36mjaccard_google_search\u001b[0;34m(search_term1, search_term2, api_key, cse_id, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mnum2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnames_to_results_dict_CSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_term1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasic_google_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_term1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcse_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mnum1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'request'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'totalResults'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#note that the number is of type str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m#save this resulting value for the search term to the dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-1e43d9920f5b>\u001b[0m in \u001b[0;36mbasic_google_search\u001b[0;34m(search_term, api_key, cse_id, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbasic_google_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcse_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customsearch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeveloperKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcse_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#res is of type dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;31m#^can have more arguments, e.g. number of results to store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#print(type(res))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/oauth2client/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    840\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://www.googleapis.com/customsearch/v1?q=%22+GEORGE+RIVELL%22&cx=0&num=1&key=0&alt=json returned \"Bad Request\">"
     ]
    }
   ],
   "source": [
    "#space out requests more - want to write a cron script eventually for maybe ~6-8 requests per hour as opposed\n",
    "#to 100 per day which means 33 edges per day\n",
    "import time\n",
    "#time.sleep(1)\n",
    "\n",
    "\n",
    "#choose the first of the following two lines if it's the very start / origination of the file. Otherwise go with second\n",
    "#don't forget to save csv at the end\n",
    "dfRes = pd.DataFrame(columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\"])\n",
    "#dfRes = pd.read_csv('./data/codonor_network_queries_instance_0_5.21_highbkt_edges.csv',index_col=0)\n",
    "#^index_col: https://stackoverflow.com/questions/36519086/pandas-how-to-get-rid-of-unnamed-column-in-a-dataframe\n",
    "\n",
    "counter = start\n",
    "#savestr = './data/codonor_network_queries_instance_0_5.21_highbkt_edges.csv'\n",
    "savestr = './data/googleCSE/googleCSE_codonor_network_queries_instance_0_5.22_highbkt_edges.csv'\n",
    "#'''\n",
    "for edge in all_edges_to_sample:\n",
    "    time.sleep(1)\n",
    "    print(counter)\n",
    "    counter = counter + 1\n",
    "    name1 = '\"' + edge[0] + '\"'\n",
    "    name2 = '\"' + edge[1] + '\"'\n",
    "\n",
    "    my_api_key, my_cse_id = \"0\", \"0\"\n",
    "\n",
    "    #*********************v regular or scrape****************************\n",
    "    res = jaccard_google_search(name1, name2, my_api_key, my_cse_id, num=1)\n",
    "    #   retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    dfRow = pd.DataFrame([[edge[0], edge[1], (edge[2])['number_of_codonors'], res[0], res[1], res[2], res[3], res[4], res[5], instanceNum]], \n",
    "                       columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"divMin\", \"divProd\", \"instanceNum\"])\n",
    "    #print(dfRow)\n",
    "    #print(dfRes.head(n=1))\n",
    "    #print(dfRes.tail(n=1))\n",
    "    dfRes = dfRes.append(dfRow, ignore_index=True)\n",
    "'''\n",
    "except Exception as e:\n",
    "    print(\"error: \" + str(e))#logger.error\n",
    "    dfRes.to_csv(savestr)#note - this is cumulative for now, not just the date I'm giving in the name\n",
    "    print('saved in except')\n",
    "    #I'd like to be able to print out the exception but the block stopped working when I did that.\n",
    "'''\n",
    "print('queries done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len of ending names_to_results_dict: \")\n",
    "print(len(names_to_results_dict_CSE))\n",
    "dfRes.to_csv(savestr)\n",
    "print('saved no error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
