{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path \n",
    "req_link = '/Users/ravirao/anaconda/lib/python3.5/site-packages/python-louvain-0.3'\n",
    "sys.path.append(req_link) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys #only needed to determine Python version number\n",
    "import matplotlib as plot#only needed to determine Matplotlib version number\n",
    "import os, datetime\n",
    "import re\n",
    "import networkx as nx\n",
    "#import community\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./Documents/donation_data/contri_all_2012_clean.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-681df942a231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0;34m'winner'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                         'ran.general':pd.np.float64,},\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n/a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                   )\n",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arjunrao/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./Documents/donation_data/contri_all_2012_clean.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#read the main data#replaced 2004 w 2012\n",
    "df = pd.read_csv('./Documents/donation_data/contri_all_2012_clean.csv', sep=',', header=0,\n",
    "                 usecols= [\"contributor_name\",\"contributor_type\",'amount','recipient_name',\n",
    "                           'recipient_state','seat','recipient_type','recipient_party','contributor_cfscore',\n",
    "                           'candidate_cfscore','winner','ran.general'],\n",
    "                 dtype={'contributor_name':str,\n",
    "                        'contributor_type': str,\n",
    "                        'amount':pd.np.float64,\n",
    "                        'recipient_name':str,\n",
    "                        'recipient_state':str,\n",
    "                        'seat': str,\n",
    "                        'recipient_type':str,\n",
    "                        'recipient_party':str,\n",
    "                        'contributor_cfscore':pd.np.float64,\n",
    "                        'candidate_cfscore':pd.np.float64,\n",
    "                        'winner':str,\n",
    "                        'ran.general':pd.np.float64,},\n",
    "                    na_values=['n/a']\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'ran.general': 'ran_general'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['ran_general']==1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['contributor_type']==\"I\"]#Individual vs. C for committee/org\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1=df.groupby('recipient_name')\n",
    "df_group1=group1.agg({'amount' : np.sum, 'contributor_name' : lambda x: \"%s\" % ',, '.join(x)})\n",
    "df_group1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group1=df_group1.reset_index()\n",
    "print(len(df_group1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df.drop(['amount','contributor_name'],1)\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house_main=pd.merge(df_copy, df_group1, on='recipient_name', how='inner')\n",
    "df_house_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup=df_house_main.drop_duplicates(['recipient_name','recipient_state','winner',\n",
    "                                        'recipient_party'])\n",
    "df_no_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup=df_no_dup.reset_index()\n",
    "df_no_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup=df_no_dup.drop('index',1)\n",
    "df_no_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_no_dup\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contributor_set(row):\n",
    "    name=row['contributor_name']\n",
    "    return list(set(name.split(',,')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.apply(clean_contributor_set, axis=1)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=pd.DataFrame(y, columns=['unique_codonors'])\n",
    "y1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,y1], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_unique_codonors(row):\n",
    "    return len(row['unique_codonors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2=df.apply(number_unique_codonors, axis=1)\n",
    "y2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3=pd.DataFrame(y2, columns=['number_unique_codonors'])\n",
    "y3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,y3], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the race file and joining it with prev dataframe\n",
    "df_race = pd.read_csv('./Documents/donation_data/candidate_race_distance.csv', sep=',', header=0,\n",
    "                 usecols= ['year','name','dis_opp'],\n",
    "                 dtype={'name':str,\n",
    "                        'year':pd.np.float64,\n",
    "                        'dis_opp':pd.np.float64},\n",
    "                    na_values=['n/a']\n",
    "                  )\n",
    "df_race.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_race=df_race[df_race['year']==2012]#replaced 2004 w 2012\n",
    "df_race.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_race=df_race[np.isfinite(df_race['dis_opp'])]#isfinite removes inf and NaN\n",
    "df_race.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=list(df['recipient_name'])\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_race=list(df_race['name'])\n",
    "print(len(names_race))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common=[]\n",
    "for i in range(len(names)):\n",
    "    if names[i] in names_race:\n",
    "        common.append(names[i])\n",
    "        \n",
    "print(len(common))\n",
    "print(len(set(common)))\n",
    "common = set(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common=df.loc[df['recipient_name'].isin(common)]\n",
    "print(len(common))\n",
    "print(len(df_common))\n",
    "\n",
    "\n",
    "df_common=df_common.drop_duplicates(subset='recipient_name', keep=\"last\")\n",
    "print(len(df_common))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_common=df_race.loc[df_race['name'].isin(common)]\n",
    "print(len(race_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#race_common=df_race.loc[df_race['name'].isin(common)]\n",
    "race_common=race_common.drop_duplicates(subset='name', keep=\"last\")\n",
    "print(len(race_common))\n",
    "race_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_common['dis_opp']=abs(race_common['dis_opp'])\n",
    "race_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_common=race_common.reset_index()\n",
    "race_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_common=race_common.drop(['index','year'],1)\n",
    "race_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_common.rename(columns={'name': 'recipient_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.merge(race_common, df_common, on='recipient_name', how='inner')\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the state-ideo map data to join it with previous dataframe\n",
    "df_region = pd.read_csv('./Documents/donation_data/candidate_race_distance_state_ideology.csv', sep=',', header=0,\n",
    "                 usecols= ['year','name','class'],\n",
    "                 dtype={'name':str,\n",
    "                        'year':pd.np.float64,\n",
    "                        'class':str},\n",
    "                    na_values=['n/a']\n",
    "                  )\n",
    "df_region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region=df_region[df_region['year']==2012]#replaced 2004 w 2012\n",
    "print(len(df_region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_common=df_region.loc[df_region['name'].isin(common)]\n",
    "print(len(region_common))\n",
    "region_common=region_common.drop_duplicates(subset='name', keep=\"last\")\n",
    "print(len(region_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_common.rename(columns={'name': 'recipient_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_common=region_common.reset_index()\n",
    "region_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_common=region_common.drop(['index','year'],1)\n",
    "region_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main1=pd.merge(final, region_common, on='recipient_name', how='inner')\n",
    "main1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main=main1.drop(['contributor_name'],1)\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.rename(columns={'class': 'region'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing data as a csv or txt file (I ran both so both files exist in JieGaoResearch)\n",
    "#the csv file doesn't come out right bc it assumes comma separated. Would need to change the 'sep' tag too to make that work\n",
    "main.to_csv(path_or_buf='new_individual_donors_2012.txt',header=True, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main=main.reset_index()\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main=main.drop(['index'],1)\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1=nx.Graph()\n",
    "\n",
    "main = main.fillna('')\n",
    "\n",
    "#import pdb\n",
    "\n",
    "for i in range(0, len(main)):\n",
    "    print(\"Node no: \", i)\n",
    "    recipient_contributors_1=main.iloc[i]['unique_codonors']\n",
    "    recipient_number_of_contributers_1=int(main.iloc[i]['number_unique_codonors'])\n",
    "    recipient_name_1=main.iloc[i]['recipient_name']\n",
    "    recipient_amount_1=int(main.iloc[i]['amount'])\n",
    "    recipient_party_1=main.iloc[i]['recipient_party']\n",
    "    recipient_region_1=main.iloc[i]['region']\n",
    "    recipient_win_1=str(main.iloc[i]['winner'])\n",
    "    recipient_ideo_1=float(main.iloc[i]['candidate_cfscore'])\n",
    "    recipient_race_1=float(main.iloc[i]['dis_opp'])\n",
    "    \n",
    "    if recipient_contributors_1 == '' or recipient_name_1 == '' or recipient_number_of_contributers_1<1000:\n",
    "        continue\n",
    "    \n",
    "    if recipient_name_1 in H1:\n",
    "        for j in range(0, len(main)):\n",
    "            recipient_contributors_2=main.iloc[j]['unique_codonors']\n",
    "            recipient_number_of_contributers_2=int(main.iloc[j]['number_unique_codonors'])\n",
    "            recipient_name_2=main.iloc[j]['recipient_name']\n",
    "            recipient_amount_2=int(main.iloc[j]['amount'])\n",
    "            recipient_region_2=main.iloc[j]['region']\n",
    "            recipient_party_2=main.iloc[j]['recipient_party']\n",
    "            recipient_win_2=str(main.iloc[j]['winner'])\n",
    "            recipient_ideo_2=float(main.iloc[j]['candidate_cfscore'])\n",
    "            recipient_race_2=float(main.iloc[j]['dis_opp'])\n",
    "\n",
    "            if recipient_contributors_2 == '' or recipient_name_2 == '' or recipient_number_of_contributers_2<1000:\n",
    "                continue\n",
    "\n",
    "            if recipient_name_2 in H1:\n",
    "                \n",
    "                if H1.has_edge(recipient_name_1, recipient_name_2):\n",
    "                    continue\n",
    "                else:\n",
    "                    common=set(recipient_contributors_1).intersection(recipient_contributors_2)\n",
    "                    num_codonors=int(len(set(recipient_contributors_1).intersection(recipient_contributors_2)))\n",
    "                    ratio_1=float(num_codonors/recipient_number_of_contributers_1)\n",
    "                    ratio_2=float(num_codonors/recipient_number_of_contributers_2)\n",
    "                    if min(ratio_1,ratio_2)>0.1:\n",
    "                        H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                    elif min(ratio_1,ratio_2)<=0.1:\n",
    "                        H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                        #pdb.set_trace()\n",
    "                        #print(\"No edge!\")\n",
    "           \n",
    "            elif recipient_name_2 not in H1:\n",
    "                H1.add_node(recipient_name_2, name=recipient_name_2,race=recipient_race_2,\n",
    "                           win=recipient_win_2, number_of_donors=recipient_number_of_contributers_2,\n",
    "                           amount=recipient_amount_2, party=recipient_party_2, ideo=recipient_ideo_2,\n",
    "                           region=recipient_region_2)\n",
    "                common=set(recipient_contributors_1).intersection(recipient_contributors_2)\n",
    "                num_codonors=int(len(set(recipient_contributors_1).intersection(recipient_contributors_2)))\n",
    "                ratio_1=float(num_codonors/recipient_number_of_contributers_1)\n",
    "                ratio_2=float(num_codonors/recipient_number_of_contributers_2)\n",
    "                if min(ratio_1,ratio_2)>0.1:\n",
    "                    H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                elif min(ratio_1,ratio_2)<=0.1:\n",
    "                    H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                    #pdb.set_trace()\n",
    "                    #print(\"No edge!\")\n",
    "        \n",
    "    elif recipient_name_1 not in H1:\n",
    "        H1.add_node(recipient_name_1, name=recipient_name_1, win=recipient_win_1, race=recipient_race_1,\n",
    "               number_of_donors=recipient_number_of_contributers_1, amount=recipient_amount_1, \n",
    "                party=recipient_party_1,ideo=recipient_ideo_1,region=recipient_region_1)\n",
    "        \n",
    "        if recipient_number_of_contributers_1<1000:\n",
    "                continue\n",
    "    \n",
    "        for j in range(0, len(main)):\n",
    "            recipient_contributors_2=main.iloc[j]['unique_codonors']\n",
    "            recipient_number_of_contributers_2=int(main.iloc[j]['number_unique_codonors'])\n",
    "            recipient_name_2=main.iloc[j]['recipient_name']\n",
    "            recipient_amount_2=int(main.iloc[j]['amount'])\n",
    "            recipient_party_2=main.iloc[j]['recipient_party']\n",
    "            recipient_region_2=main.iloc[j]['region']\n",
    "            recipient_win_2=str(main.iloc[j]['winner'])\n",
    "            recipient_ideo_2=float(main.iloc[j]['candidate_cfscore'])\n",
    "            recipient_race_2=float(main.iloc[j]['dis_opp'])\n",
    "\n",
    "            if recipient_contributors_2 == '' or recipient_name_2 == '' or recipient_number_of_contributers_2<1000:\n",
    "                continue\n",
    "\n",
    "            if recipient_name_2 in H1:\n",
    "                if H1.has_edge(recipient_name_1, recipient_name_2):\n",
    "                    continue\n",
    "                else:\n",
    "                    common=set(recipient_contributors_1).intersection(recipient_contributors_2)\n",
    "                    num_codonors=int(len(set(recipient_contributors_1).intersection(recipient_contributors_2)))\n",
    "                    ratio_1=float(num_codonors/recipient_number_of_contributers_1)\n",
    "                    ratio_2=float(num_codonors/recipient_number_of_contributers_2)\n",
    "                    if min(ratio_1,ratio_2)>0.1:\n",
    "                        H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                    elif min(ratio_1,ratio_2)<=0.1:\n",
    "                        H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                        #pdb.set_trace()\n",
    "                        #print(\"No edge!\")\n",
    "           \n",
    "            elif recipient_name_2 not in H1:\n",
    "                H1.add_node(recipient_name_2, name=recipient_name_2,race=recipient_race_2,region=recipient_region_2,\n",
    "                           win=recipient_win_2, number_of_donors=recipient_number_of_contributers_2,\n",
    "                           amount=recipient_amount_2,party=recipient_party_2, ideo=recipient_ideo_2)\n",
    "                \n",
    "                common=set(recipient_contributors_1).intersection(recipient_contributors_2)\n",
    "                num_codonors=int(len(set(recipient_contributors_1).intersection(recipient_contributors_2)))\n",
    "                ratio_1=float(num_codonors/recipient_number_of_contributers_1)\n",
    "                ratio_2=float(num_codonors/recipient_number_of_contributers_2)\n",
    "                if min(ratio_1,ratio_2)>0.1:\n",
    "                    H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                elif min(ratio_1,ratio_2)<=0.1:\n",
    "                    #pdb.set_trace()\n",
    "                    H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                    print(\"No edge!\")\n",
    "#pdb.set_trace()\n",
    "print(nx.number_of_nodes(H1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.number_of_nodes(H1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( nx.number_of_edges(H1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self=H1.selfloop_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1.remove_edges_from(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.number_of_edges(H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_edgelist(H1, 'co_donor.edgelist')\n",
    "print('written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#codonor graph - nodes are recipients. Edges are number of donors in common.\n",
    "H1 = nx.read_edgelist('../co_donor_relabeled_nodes.txt',nodetype=str,delimiter='\\t!\\t')#data=(('number_of_codonors',int)))\n",
    "\n",
    "print(nx.number_of_nodes(H1) )#should be 212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_codonors_list = []\n",
    "for e in H1.edges(data=True):\n",
    "    #e is a tuple\n",
    "    temp = (e[2])['number_of_codonors']\n",
    "    num_codonors_list.append(temp)\n",
    "    if type(temp) != int:\n",
    "        print('str:',temp)\n",
    "        \n",
    "num_codonors_list = sorted(num_codonors_list)\n",
    "\n",
    "#want to plot distribution of num_codonors, each element of which is a value in the affinity matrix\n",
    "\n",
    "'''#this part of the code started bugging out even though I didn't change anything\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = sorted(num_codonors_list)\n",
    "bins = 10**(np.arange(0,10))\n",
    "print (\"bins: \", bins)\n",
    "plt.xscale('log')\n",
    "plt.hist(data,bins=bins) \n",
    "'''\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "print('done')\n",
    "#np.logspace(np.log10(0.1),np.log10(1.0),50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_edges = [] \n",
    "for e in H1.edges(data=True):\n",
    "    temp = (e[2])['number_of_codonors']\n",
    "    #^elements 0 and 1 are the names of the endpoint nodes, element 2 is a dict of the data\n",
    "    list_of_edges.append(e)\n",
    "list_of_edges = sorted(list_of_edges, key=lambda tup: tup[2]['number_of_codonors'])#sort by number of codonors\n",
    "\n",
    "num_edges = len(list_of_edges)\n",
    "print(\"num_edges:\")\n",
    "print(num_edges)\n",
    "print(float(.05*num_edges))\n",
    "high_bkt = []#all edges above the 5% mark\n",
    "high_cutoff = num_edges - float(.05*num_edges)\n",
    "mid_bkt = []\n",
    "mid_cutoff = num_edges - float(.3*num_edges)\n",
    "low_bkt = []\n",
    "\n",
    "for e in range(num_edges):\n",
    "    if e >= high_cutoff:\n",
    "        high_bkt.append(list_of_edges[e])\n",
    "    elif e >= mid_cutoff:\n",
    "        mid_bkt.append(list_of_edges[e])\n",
    "    else:\n",
    "        low_bkt.append(list_of_edges[e])\n",
    "        \n",
    "#print(high_bucket)\n",
    "print(\"high_bkt len:\")\n",
    "print(len(high_bkt))\n",
    "print(\"mid_bkt len:\")\n",
    "print(len(mid_bkt))\n",
    "print(\"low_bkt len:\")\n",
    "print(len(low_bkt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample say 50 from each bucket. Then google query code will do 150 edges = 450 queries. Test a couple first.\n",
    "#random_nums = np.random.choice(len(high_bkt), 50, replace=False)#no replacement- want unique ones.\n",
    "#let's just randomly sample the entire thing as a list, then take the first 50 - in case we want to do more later.\n",
    "random_nums = np.random.choice(len(high_bkt), len(high_bkt), replace=False)#sampled the ENTIRE thing randomly\n",
    "sprint(high_50)\n",
    "random_nums2 = np.random.choice(len(mid_bkt), len(mid_bkt), replace=False)\n",
    "mid_50 = random_nums2[:33]\n",
    "print(mid_50)\n",
    "random_nums3 = np.random.choice(len(low_bkt), len(low_bkt), replace=False)\n",
    "low_50 = random_nums3[:33]\n",
    "print(low_50)\n",
    "\n",
    "high_edges_to_sample = []\n",
    "mid_edges_to_sample = []\n",
    "low_edges_to_sample = []\n",
    "\n",
    "for i in high_50:\n",
    "    high_edges_to_sample.append(high_bkt[i])\n",
    "for j in mid_50:\n",
    "    mid_edges_to_sample.append(mid_bkt[i])\n",
    "for k in low_50:\n",
    "    low_edges_to_sample.append(low_bkt[i])\n",
    "    \n",
    "#all_edges_to_sample = high_edges_to_sample + mid_edges_to_sample + low_edges_to_sample\n",
    "all_edges_to_sample = high_bkt[0:100]\n",
    "print(len(all_edges_to_sample))\n",
    "\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for methods for performing the google queries- see JaccardQueriesV2.ipynb\n",
    "#did ulimit -n 2048 in terminal (actually 512)\n",
    "#see https://stackoverflow.com/questions/39537731/errno-24-too-many-open-files-but-i-am-not-opening-files\n",
    "#OSError: [Errno 24] Too many open files: '/Users/ravirao/anaconda/lib/python3.5/site-packages/googleapiclient/__init__.py'\n",
    "#Turns out the error was on their side, their server\n",
    "\n",
    "#V2 returns a list, and has a method to print out the contents of the list.\n",
    "#V3 (or maybe i'll do it in this doc) will be the one I will prob use overall - read a csv file,\n",
    "#randomly decide which ppl to take,\n",
    "#check if they've already been done before querying each of the 3 possibilities, etc.\n",
    "\n",
    "#this link was very helpful:\n",
    "#https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "#I was having trouble creating the custom search engine in the first place, as detailed more in my notes\n",
    "#but I got it, and an API key, and now I think I can just use the API in python\n",
    "import sys\n",
    "sys.version\n",
    "print(sys.version_info)\n",
    "#this is python 3.5, so why can't I access the googleapiclient module? I ran the command:\n",
    "#pip install --upgrade google-api-python-client\n",
    "#so it should work\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "import pprint\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "#Search engine ID for the custom search engine I created (Political Candidates Jaccard Index Queries):\n",
    "#017540791571327827296:xy6h11dupmm\n",
    "\n",
    "#The API key is different; I thought it would have to be connected with my custom search engine upon generation but it wasn't\n",
    "#Oh, I have to use my search engine ID as input as well.\n",
    "\n",
    "my_api_key = \"AIzaSyDNjEEUVZNXM_EcKoYmITcuoiZEEq6hQUs\"\n",
    "my_cse_id = \"005060056456624332840:lxtmrbo6eho\"\n",
    "\n",
    "#These three methods will assume you've already put quotes around the search terms you pass in\n",
    "\n",
    "def basic_google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()#res is of type dict\n",
    "    #^can have more arguments, e.g. number of results to store\n",
    "    #print(type(res))\n",
    "    return res['queries']\n",
    "    #return res['queries']['request'][0]['totalResults'] #note that the number is of type str\n",
    "    #^commented out bc want more info available potentially - can be more specific for jaccard\n",
    "    \n",
    "    #https://developers.google.com/custom-search/json-api/v1/reference/cse/list #how list method works\n",
    "    \n",
    "def exact_google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, exactTerms=search_term, cx=cse_id, **kwargs).execute()#res is of type dict\n",
    "    #^can have more arguments, e.g. number of results to store\n",
    "    #print(type(res))\n",
    "    return res['queries']\n",
    "\n",
    "\n",
    "def jaccard_google_search(search_term1, search_term2, api_key, cse_id, **kwargs):\n",
    "    \n",
    "    results = basic_google_search(search_term1, api_key, cse_id, **kwargs)\n",
    "    num1 = results['request'][0]['totalResults'] #note that the number is of type str\n",
    "    results = basic_google_search(search_term2, api_key, cse_id, **kwargs)\n",
    "    num2 = results['request'][0]['totalResults'] #note that the number is of type str\n",
    "    \n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    strBoth = search_term1 + ' ' + search_term2\n",
    "    #how do you put multiple phrases in as exactTerms? idk if that's possible... #outdated comment\n",
    "    results = basic_google_search(strBoth, api_key, cse_id, **kwargs)\n",
    "    numCommon = results['request'][0]['totalResults'] #note that the number is of type str\n",
    "    \n",
    "    num1 = int(num1)\n",
    "    num2 = int(num2)\n",
    "    numCommon = int(numCommon)\n",
    "    #print('num results for str1:' + str(num1) )\n",
    "    #print('num results for str2:' + str(num2) )\n",
    "    #print('num results in common:' + str(numCommon) )\n",
    "    \n",
    "    #sometimes denominator was 0. try except ZeroDivionError.\n",
    "    try:\n",
    "        jacInd = numCommon /(num1+num2-numCommon) #can still use / for regular division, but want decimals here.\n",
    "    except ZeroDivisionError:\n",
    "        jacInd = -1\n",
    "    \n",
    "    try:\n",
    "        divMin = numCommon / min(num1, num2)\n",
    "    except ZeroDivisionError:\n",
    "        divMin = -1\n",
    "    try:\n",
    "        divProd = numCommon / (num1*num2)\n",
    "    except:\n",
    "        divProd = -1\n",
    "    #retList is of format: num results for str1, num results for str2, num results in common, the Jaccard index,\n",
    "    #the intersection / the min, and the intersection / the product\n",
    "    retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    return retList\n",
    "\n",
    "print('done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#host google.com 8.8.8.8\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#had to write a scraping method to work for me until I figure out why I can't find the server...\n",
    "def jaccard_google_search_scrape(search_term1, search_term2, api_key, cse_id, **kwargs):\n",
    "\n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':search_term1})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    num1 = int(res.text.replace(\",\", \"\").split()[1])\n",
    "\n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':search_term2})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    num2 = int(res.text.replace(\",\", \"\").split()[1])\n",
    "    \n",
    "    strBoth = search_term1 + ' ' + search_term2\n",
    "    \n",
    "    r = requests.get(\"https://www.google.com/search\", params={'q':strBoth})\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    res = soup.find(\"div\", {\"id\": \"resultStats\"})\n",
    "    numCommon = int(res.text.replace(\",\", \"\").split()[1])\n",
    "    \n",
    "    num1 = int(num1)\n",
    "    num2 = int(num2)\n",
    "    numCommon = int(numCommon)\n",
    "    #print('num results for str1:' + str(num1) )\n",
    "    #print('num results for str2:' + str(num2) )\n",
    "    #print('num results in common:' + str(numCommon) )\n",
    "    \n",
    "    \n",
    "    #sometimes denominator was 0. try except ZeroDivionError.\n",
    "    try:\n",
    "        jacInd = numCommon /(num1+num2-numCommon) #can still use / for regular division, but want decimals here.\n",
    "    except ZeroDivisionError:\n",
    "        jacInd = -1\n",
    "    \n",
    "    try:\n",
    "        divMin = numCommon / min(num1, num2)\n",
    "    except ZeroDivisionError:\n",
    "        divMin = -1\n",
    "    try:\n",
    "        divProd = numCommon / (num1*num2)\n",
    "    except:\n",
    "        divProd = -1\n",
    "    #retList is of format: num results for str1, num results for str2, num results in common, the Jaccard index,\n",
    "    #the intersection / the min, and the intersection / the product\n",
    "    retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#choose the first of the following two lines if it's the very start / origination of the file. Otherwise go with second\n",
    "dfRes = pd.DataFrame(columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"var1\", \"var2\"])\n",
    "#dfRes = pd.read_csv('./co_donor_google_queries_fin_6.csv')\n",
    "\n",
    "counter = 0\n",
    "#'''\n",
    "#try:\n",
    "for edge in all_edges_to_sample:\n",
    "    print(counter)\n",
    "    counter = counter + 1\n",
    "    name1 = '\"' + edge[0] + '\"'\n",
    "    name2 = '\"' + edge[1] + '\"'\n",
    "\n",
    "    res = jaccard_google_search(name1, name2, my_api_key, my_cse_id, num=1)\n",
    "    #   retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    dfRow = pd.DataFrame([[edge[0], edge[1], (edge[2])['number_of_codonors'], res[0], res[1], res[2], res[3], res[4], res[5]]], \n",
    "                       columns = [\"str1\", \"str2\", \"num_codonors\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"var1\", \"var2\"])\n",
    "\n",
    "    dfRes = dfRes.append(dfRow, ignore_index=True)\n",
    "#'''\n",
    "#except:\n",
    "#dfRes.to_csv('./co_donor_google_queries_fin_6.csv')\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRes.to_csv('./co_donor_google_queries_fin_2018_test1.csv')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.write_edgelist(H1, \"co_donor_test.txt\", delimiter='\\t!\\t')\n",
    "#print('written')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
