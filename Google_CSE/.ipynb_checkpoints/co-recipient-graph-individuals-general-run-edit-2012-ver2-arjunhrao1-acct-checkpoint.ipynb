{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path \n",
    "req_link = '/Users/ravirao/anaconda/lib/python3.5/site-packages/python-louvain-0.3'\n",
    "sys.path.append(req_link) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys #only needed to determine Python version number\n",
    "import matplotlib as plot#only needed to determine Matplotlib version number\n",
    "import os, datetime\n",
    "import re\n",
    "import networkx as nx\n",
    "#import community\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File b'./Documents/donation_data/contri_all_2012_clean.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-681df942a231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0;34m'winner'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                         'ran.general':pd.np.float64,},\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n/a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                   )\n",
      "\u001b[0;32m/Users/ravirao/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ravirao/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ravirao/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ravirao/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ravirao/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File b'./Documents/donation_data/contri_all_2012_clean.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#read the main data#replaced 2004 w 2012\n",
    "df = pd.read_csv('./Documents/donation_data/contri_all_2012_clean.csv', sep=',', header=0,\n",
    "                 usecols= [\"contributor_name\",\"contributor_type\",'amount','recipient_name',\n",
    "                           'recipient_state','seat','recipient_type','recipient_party','contributor_cfscore',\n",
    "                           'candidate_cfscore','winner','ran.general'],\n",
    "                 dtype={'contributor_name':str,\n",
    "                        'contributor_type': str,\n",
    "                        'amount':pd.np.float64,\n",
    "                        'recipient_name':str,\n",
    "                        'recipient_state':str,\n",
    "                        'seat': str,\n",
    "                        'recipient_type':str,\n",
    "                        'recipient_party':str,\n",
    "                        'contributor_cfscore':pd.np.float64,\n",
    "                        'candidate_cfscore':pd.np.float64,\n",
    "                        'winner':str,\n",
    "                        'ran.general':pd.np.float64,},\n",
    "                    na_values=['n/a']\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'ran.general': 'ran_general'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=df[df['ran_general']==1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=df[df['contributor_type']==\"I\"]#Individual vs. C for committee/org\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group1=df.groupby('contributor_name')\n",
    "df_group1=group1.agg({'amount' : np.sum, 'recipient_name' : lambda x: \"%s\" % ',, '.join(x)})\n",
    "df_group1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_group1=df_group1.reset_index()\n",
    "print(len(df_group1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy=df.drop(['amount','recipient_name'],1)\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_house_main=pd.merge(df_copy, df_group1, on='contributor_name', how='inner')\n",
    "df_house_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_no_dup=df_house_main.drop_duplicates(['contributor_name'])#,'recipient_state','winner', 'recipient_party'])\n",
    "df_no_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_no_dup=df_no_dup.reset_index()\n",
    "df_no_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_no_dup=df_no_dup.drop('index',1)\n",
    "df_no_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=df_no_dup\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_recipient_set(row):\n",
    "    name=row['recipient_name']\n",
    "    return list(set(name.split(',,')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=df.apply(clean_recipient_set, axis=1)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y1=pd.DataFrame(y, columns=['unique_corecips'])\n",
    "y1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df,y1], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_unique_corecips(row):\n",
    "    return len(row['unique_corecips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y2=df.apply(number_unique_corecips, axis=1)\n",
    "y2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y3=pd.DataFrame(y2, columns=['number_unique_corecips'])\n",
    "y3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df,y3], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read the race file and joining it with prev dataframe\n",
    "#This is the co recipient graph, so forget the race file - need to use the donor file.\n",
    "df_race = pd.read_csv('./Documents/donation_data/donor_list_2012_clean.csv', sep=',', header=0,\n",
    "                 usecols= ['contributor_name'],\n",
    "                 dtype={'name':str,},\n",
    "                    na_values=['n/a']\n",
    "                  )\n",
    "df_race.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#it's already the 2012 list\n",
    "\n",
    "#df_race=df_race[df_race['year']==2012]#replaced 2004 w 2012\n",
    "#df_race.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_race=df_race[np.isfinite(df_race['dis_opp'])]\n",
    "#df_race.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names=list(df['contributor_name'])\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names_race=list(df_race['contributor_name'])\n",
    "print(len(names_race))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common = list(set(names) & set(names_race))\n",
    "        \n",
    "print(len(common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_common=df.loc[df['contributor_name'].isin(common)]\n",
    "print(len(common))\n",
    "print(len(df_common))\n",
    "\n",
    "\n",
    "df_common=df_common.drop_duplicates(subset='contributor_name', keep=\"last\")\n",
    "print(len(df_common))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "race_common=df_race.loc[df_race['contributor_name'].isin(common)]\n",
    "race_common=race_common.drop_duplicates(subset='contributor_name', keep=\"last\")\n",
    "print(len(race_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "race_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#race_common['dis_opp']=abs(race_common['dis_opp'])\n",
    "#race_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "race_common=race_common.reset_index()\n",
    "race_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "race_common=race_common.drop(['index'],1)\n",
    "race_common.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "race_common.rename(columns={'name': 'contributor_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final=pd.merge(race_common, df_common, on='contributor_name', how='inner')\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading the state-ideo map data to join it with previous dataframe\n",
    "'''\n",
    "df_region = pd.read_csv('./Documents/donation_data/candidate_race_distance_state_ideology.csv', sep=',', header=0,\n",
    "                 usecols= ['year','name','class'],\n",
    "                 dtype={'name':str,\n",
    "                        'year':pd.np.float64,\n",
    "                        'class':str},\n",
    "                    na_values=['n/a']\n",
    "                  )\n",
    "df_region.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df_region=df_region[df_region['year']==2012]#replaced 2004 w 2012\n",
    "print(len(df_region))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "region_common=df_region.loc[df_region['name'].isin(common)]\n",
    "print(len(region_common))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "region_common.rename(columns={'name': 'recipient_name'}, inplace=True)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "region_common=region_common.reset_index()\n",
    "region_common.head()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "region_common=region_common.drop(['index','year'],1)\n",
    "region_common.head()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#don't need third merge\n",
    "main1=final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main=main1.drop(['recipient_name'],1)\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#main.rename(columns={'class': 'region'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#writing data as a csv or txt file (I ran both so both files exist in JieGaoResearch)\n",
    "#the csv file doesn't come out right bc it assumes comma separated. Would need to change the 'sep' tag too to make that work\n",
    "main.to_csv(path_or_buf='new_individual_recipients_2012.txt',header=True, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main=main.reset_index()\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main=main.drop(['index'],1)\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#There were simply way too many contributors. Made this run suuuper slow since it needs to check edges with all of the\n",
    "#other nodes. Need to filter out BEFOREHAND, not during, all of the low values for number_unique_corecips.\n",
    "main= main[main.number_unique_corecips >= 50]\n",
    "print(len(main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H1=nx.Graph()\n",
    "\n",
    "main = main.fillna('')\n",
    "\n",
    "import pdb\n",
    "\n",
    "for i in range(0, len(main)):\n",
    "\n",
    "    recipient_contributors_1=main.iloc[i]['unique_corecips']\n",
    "    recipient_number_of_contributers_1=int(main.iloc[i]['number_unique_corecips'])\n",
    "    recipient_name_1=main.iloc[i]['contributor_name']\n",
    "    recipient_amount_1=int(main.iloc[i]['amount'])\n",
    "    #recipient_party_1=main.iloc[i]['recipient_party']\n",
    "    #recipient_region_1=main.iloc[i]['region']\n",
    "    #recipient_win_1=str(main.iloc[i]['winner'])\n",
    "    #Wrote the following bc I was getting - ValueError: could not convert string to float: \n",
    "    #but now I'm getting another error... so I'll change it to just be the column, with an occassional str\n",
    "    \n",
    "    recipient_ideo_1=main.iloc[i]['contributor_cfscore']\n",
    "    #try 2: recipient_ideo_1 = main.iloc[i]['contributor_cfscore'].apply(lambda x: float(x.split()[0].replace(',', '')))\n",
    "    #try 1: recipient_ideo_1=float(main.iloc[i]['contributor_cfscore'])\n",
    "    #recipient_race_1=float(main.iloc[i]['dis_opp'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''#These are the names they should be, but not gonna change them all rn. Just the literal rightmost parts.\n",
    "    contributor_recips_1=main.iloc[i]['unique_corecips']\n",
    "    contributor_number_of_recips_1=int(main.iloc[i]['number_unique_corecips'])\n",
    "    contributor_name_1=main.iloc[i]['contributor_name']\n",
    "    contributor_amount_1=int(main.iloc[i]['amount'])\n",
    "    #contributor_party_1=main.iloc[i]['recipient_party']\n",
    "    #contributor_region_1=main.iloc[i]['region']\n",
    "    #contributor_win_1=str(main.iloc[i]['winner'])\n",
    "    contributor_ideo_1=float(main.iloc[i]['candidate_cfscore'])\n",
    "    #recipient_race_1=float(main.iloc[i]['dis_opp'])\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(\"Node no: \", i, recipient_number_of_contributers_1)\n",
    "    if recipient_number_of_contributers_1>=50:\n",
    "        print(\"High val, Add node no: \", i, recipient_number_of_contributers_1)\n",
    "        \n",
    "    if recipient_number_of_contributers_1<50 or recipient_contributors_1 == '' or recipient_name_1 == '':\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    if recipient_name_1 in H1:\n",
    "        for j in range(0, len(main)):\n",
    "            \n",
    "            recipient_contributors_2=main.iloc[j]['unique_corecips']\n",
    "            recipient_number_of_contributers_2=int(main.iloc[j]['number_unique_corecips'])\n",
    "            recipient_name_2=main.iloc[j]['contributor_name']\n",
    "            recipient_amount_2=int(main.iloc[j]['amount'])\n",
    "            #recipient_region_2=main.iloc[j]['region']\n",
    "            #recipient_party_2=main.iloc[j]['recipient_party']\n",
    "            #recipient_win_2=str(main.iloc[j]['winner'])\n",
    "            \n",
    "            recipient_ideo_2=main.iloc[i]['contributor_cfscore']\n",
    "            #recipient_ideo_2 = main.iloc[i]['contributor_cfscore'].apply(lambda x: float(x.split()[0].replace(',', '')))\n",
    "            #recipient_race_2=float(main.iloc[j]['dis_opp'])\n",
    "\n",
    "            if recipient_contributors_2 == '' or recipient_name_2 == '' or recipient_number_of_contributers_2<50:\n",
    "                continue\n",
    "\n",
    "            if recipient_name_2 in H1:\n",
    "                \n",
    "                if H1.has_edge(recipient_name_1, recipient_name_2):\n",
    "                    continue\n",
    "                else:\n",
    "                    common=set(recipient_contributors_1).intersection(recipient_contributors_2)\n",
    "                    num_codonors=int(len(set(recipient_contributors_1).intersection(recipient_contributors_2)))\n",
    "                    ratio_1=float(num_codonors/recipient_number_of_contributers_1)\n",
    "                    ratio_2=float(num_codonors/recipient_number_of_contributers_2)\n",
    "                    if min(ratio_1,ratio_2)>0.1:\n",
    "                        H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                    elif min(ratio_1,ratio_2)<=0.1:\n",
    "                        H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                        #pdb.set_trace()\n",
    "                        #print(\"No edge!\")\n",
    "           \n",
    "            elif recipient_name_2 not in H1:\n",
    "                #deleted some of these fields in add_node\n",
    "                H1.add_node(recipient_name_2, name=recipient_name_2,\n",
    "                           number_of_donors=recipient_number_of_contributers_2,\n",
    "                           amount=recipient_amount_2, ideo=recipient_ideo_2\n",
    "                           )\n",
    "                common=set(recipient_contributors_1).intersection(recipient_contributors_2)\n",
    "                num_codonors=int(len(set(recipient_contributors_1).intersection(recipient_contributors_2)))\n",
    "                ratio_1=float(num_codonors/recipient_number_of_contributers_1)\n",
    "                ratio_2=float(num_codonors/recipient_number_of_contributers_2)\n",
    "                if min(ratio_1,ratio_2)>0.1:\n",
    "                    H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                elif min(ratio_1,ratio_2)<=0.1:\n",
    "                    H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                    #pdb.set_trace()\n",
    "                    #print(\"No edge!\")\n",
    "        \n",
    "    elif recipient_name_1 not in H1:\n",
    "        H1.add_node(recipient_name_1, name=recipient_name_1,\n",
    "                           number_of_donors=recipient_number_of_contributers_1,\n",
    "                           amount=recipient_amount_1, ideo=recipient_ideo_1\n",
    "                           )\n",
    "        \n",
    "        if recipient_number_of_contributers_1<50:\n",
    "                continue\n",
    "    \n",
    "        for j in range(0, len(main)):\n",
    "            recipient_contributors_2=main.iloc[j]['unique_corecips']\n",
    "            recipient_number_of_contributers_2=int(main.iloc[j]['number_unique_corecips'])\n",
    "            recipient_name_2=main.iloc[j]['contributor_name']\n",
    "            recipient_amount_2=int(main.iloc[j]['amount'])\n",
    "            #recipient_party_2=main.iloc[j]['recipient_party']\n",
    "            #recipient_region_2=main.iloc[j]['region']\n",
    "            #recipient_win_2=str(main.iloc[j]['winner'])\n",
    "            recipient_ideo_2=main.iloc[i]['contributor_cfscore']\n",
    "            #recipient_ideo_2 = main.iloc[i]['contributor_cfscore'].apply(lambda x: float(x.split()[0].replace(',', '')))\n",
    "            #recipient_race_2=float(main.iloc[j]['dis_opp'])\n",
    "\n",
    "            if recipient_contributors_2 == '' or recipient_name_2 == '' or recipient_number_of_contributers_2<50:\n",
    "                continue\n",
    "\n",
    "            if recipient_name_2 in H1:\n",
    "                if H1.has_edge(recipient_name_1, recipient_name_2):\n",
    "                    continue\n",
    "                else:\n",
    "                    common=set(recipient_contributors_1).intersection(recipient_contributors_2)\n",
    "                    num_codonors=int(len(set(recipient_contributors_1).intersection(recipient_contributors_2)))\n",
    "                    ratio_1=float(num_codonors/recipient_number_of_contributers_1)\n",
    "                    ratio_2=float(num_codonors/recipient_number_of_contributers_2)\n",
    "                    if min(ratio_1,ratio_2)>0.1:\n",
    "                        H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                    elif min(ratio_1,ratio_2)<=0.1:\n",
    "                        H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                        #pdb.set_trace()\n",
    "                        #print(\"No edge!\")\n",
    "           \n",
    "            elif recipient_name_2 not in H1:\n",
    "                H1.add_node(recipient_name_2, name=recipient_name_2,\n",
    "                           number_of_donors=recipient_number_of_contributers_2,\n",
    "                           amount=recipient_amount_2, ideo=recipient_ideo_2\n",
    "                           )\n",
    "                \n",
    "                common=set(recipient_contributors_1).intersection(recipient_contributors_2)\n",
    "                num_codonors=int(len(set(recipient_contributors_1).intersection(recipient_contributors_2)))\n",
    "                ratio_1=float(num_codonors/recipient_number_of_contributers_1)\n",
    "                ratio_2=float(num_codonors/recipient_number_of_contributers_2)\n",
    "                if min(ratio_1,ratio_2)>0.1:\n",
    "                    H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                elif min(ratio_1,ratio_2)<=0.1:\n",
    "                    #pdb.set_trace()\n",
    "                    H1.add_edge(recipient_name_1, recipient_name_2, number_of_codonors=num_codonors)\n",
    "                    #print(\"No edge!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.number_of_nodes(H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.number_of_edges(H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self=H1.selfloop_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H1.remove_edges_from(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.number_of_edges(H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.density(H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_edgelist(H1, \"co_recip_graph.txt\", delimiter='\\t!\\t')\n",
    "print('written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_corecips_list = []\n",
    "for e in H1.edges(data=True):\n",
    "    #e is a tuple\n",
    "    temp = (e[2])['number_of_codonors']\n",
    "    num_corecips_list.append(temp)\n",
    "    if type(temp) != int:\n",
    "        print('str:',temp)\n",
    "\n",
    "#want to plot distribution of num_codonors, each element of which is a value in the affinity matrix\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = sorted(num_corecips_list)\n",
    "plt.hist(data, bins=20)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('done')\n",
    "#np.logspace(np.log10(0.1),np.log10(1.0),50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_edges = [] \n",
    "for e in H1.edges(data=True):\n",
    "    temp = (e[2])['number_of_codonors']\n",
    "    list_of_edges.append(e)\n",
    "list_of_edges = sorted(list_of_edges, key=lambda tup: tup[2]['number_of_codonors'])#sort by number of codonors\n",
    "\n",
    "num_edges = len(list_of_edges)\n",
    "print(num_edges)\n",
    "print(float(.05*num_edges))\n",
    "high_bkt = []#all edges above the 5% mark\n",
    "high_cutoff = num_edges - float(.05*num_edges)\n",
    "mid_bkt = []\n",
    "mid_cutoff = num_edges - float(.3*num_edges)\n",
    "low_bkt = []\n",
    "\n",
    "for e in range(num_edges):\n",
    "    if e >= high_cutoff:\n",
    "        high_bkt.append(list_of_edges[e])\n",
    "    elif e >= mid_cutoff:\n",
    "        mid_bkt.append(list_of_edges[e])\n",
    "    else:\n",
    "        low_bkt.append(list_of_edges[e])\n",
    "        \n",
    "#print(high_bucket)\n",
    "print(len(high_bkt))\n",
    "print(len(mid_bkt))\n",
    "print(len(low_bkt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomly sample say 50 from each bucket. Then google query code will do 150 edges = 450 queries. Test a couple first.\n",
    "#random_nums = np.random.choice(len(high_bkt), 50, replace=False)#no replacement- want unique ones.\n",
    "#let's just randomly sample the entire thing as a list, then take the first 50 - in case we want to do more later.\n",
    "random_nums = np.random.choice(len(high_bkt), len(high_bkt), replace=False)#sampled the ENTIRE thing randomly\n",
    "high_50 = random_nums[:33]#took the first 50 of the random order\n",
    "print(high_50)\n",
    "random_nums2 = np.random.choice(len(mid_bkt), len(mid_bkt), replace=False)\n",
    "mid_50 = random_nums2[:33]\n",
    "print(mid_50)\n",
    "random_nums3 = np.random.choice(len(low_bkt), len(low_bkt), replace=False)\n",
    "low_50 = random_nums3[:33]\n",
    "print(low_50)\n",
    "\n",
    "high_edges_to_sample = []\n",
    "mid_edges_to_sample = []\n",
    "low_edges_to_sample = []\n",
    "\n",
    "for i in high_50:\n",
    "    high_edges_to_sample.append(high_bkt[i])\n",
    "for j in mid_50:\n",
    "    mid_edges_to_sample.append(mid_bkt[i])\n",
    "for k in low_50:\n",
    "    low_edges_to_sample.append(low_bkt[i])\n",
    "    \n",
    "all_edges_to_sample = high_edges_to_sample + mid_edges_to_sample + low_edges_to_sample\n",
    "print(len(all_edges_to_sample))\n",
    "\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code for methods for performing the google queries- see JaccardQueriesV2.ipynb\n",
    "#did ulimit -n 2048 in terminal (actually 512)\n",
    "#see https://stackoverflow.com/questions/39537731/errno-24-too-many-open-files-but-i-am-not-opening-files\n",
    "#OSError: [Errno 24] Too many open files: '/Users/ravirao/anaconda/lib/python3.5/site-packages/googleapiclient/__init__.py'\n",
    "#Turns out the error was on their side, their server\n",
    "\n",
    "#V2 returns a list, and has a method to print out the contents of the list.\n",
    "#V3 (or maybe i'll do it in this doc) will be the one I will prob use overall - read a csv file,\n",
    "#randomly decide which ppl to take,\n",
    "#check if they've already been done before querying each of the 3 possibilities, etc.\n",
    "\n",
    "#this link was very helpful:\n",
    "#https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "#I was having trouble creating the custom search engine in the first place, as detailed more in my notes\n",
    "#but I got it, and an API key, and now I think I can just use the API in python\n",
    "import sys\n",
    "sys.version\n",
    "print(sys.version_info)\n",
    "#this is python 3.5, so why can't I access the googleapiclient module? I ran the command:\n",
    "#pip install --upgrade google-api-python-client\n",
    "#so it should work\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "import pprint\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "#Search engine ID for the custom search engine I created (Political Candidates Jaccard Index Queries):\n",
    "#017540791571327827296:xy6h11dupmm\n",
    "\n",
    "#The API key is different; I thought it would have to be connected with my custom search engine upon generation but it wasn't\n",
    "#Oh, I have to use my search engine ID as input as well.\n",
    "\n",
    "my_api_key = \"AIzaSyCNO5wrFa5RcehkMyY6rxrkjlHVFaJ4rF4\"\n",
    "my_cse_id = \"012951132720784451195:aybfyk7h8wm\"\n",
    "\n",
    "#These three methods will assume you've already put quotes around the search terms you pass in\n",
    "\n",
    "def basic_google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()#res is of type dict\n",
    "    #^can have more arguments, e.g. number of results to store\n",
    "    #print(type(res))\n",
    "    return res['queries']\n",
    "    #return res['queries']['request'][0]['totalResults'] #note that the number is of type str\n",
    "    #^commented out bc want more info available potentially - can be more specific for jaccard\n",
    "    \n",
    "    #https://developers.google.com/custom-search/json-api/v1/reference/cse/list #how list method works\n",
    "    \n",
    "def exact_google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, exactTerms=search_term, cx=cse_id, **kwargs).execute()#res is of type dict\n",
    "    #^can have more arguments, e.g. number of results to store\n",
    "    #print(type(res))\n",
    "    return res['queries']\n",
    "\n",
    "\n",
    "def jaccard_google_search(search_term1, search_term2, api_key, cse_id, **kwargs):\n",
    "    \n",
    "    results = basic_google_search(search_term1, api_key, cse_id, **kwargs)\n",
    "    num1 = results['request'][0]['totalResults'] #note that the number is of type str\n",
    "    results = basic_google_search(search_term2, api_key, cse_id, **kwargs)\n",
    "    num2 = results['request'][0]['totalResults'] #note that the number is of type str\n",
    "    \n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    strBoth = search_term1 + ' ' + search_term2\n",
    "    #how do you put multiple phrases in as exactTerms? idk if that's possible... #outdated comment\n",
    "    results = basic_google_search(strBoth, api_key, cse_id, **kwargs)\n",
    "    numCommon = results['request'][0]['totalResults'] #note that the number is of type str\n",
    "    \n",
    "    num1 = int(num1)\n",
    "    num2 = int(num2)\n",
    "    numCommon = int(numCommon)\n",
    "    #print('num results for str1:' + str(num1) )\n",
    "    #print('num results for str2:' + str(num2) )\n",
    "    #print('num results in common:' + str(numCommon) )\n",
    "    \n",
    "    #sometimes denominator was 0. try except ZeroDivionError.\n",
    "    try:\n",
    "        jacInd = numCommon /(num1+num2-numCommon) #can still use / for regular division, but want decimals here.\n",
    "    except ZeroDivisionError:\n",
    "        jacInd = -1\n",
    "    \n",
    "    try:\n",
    "        divMin = numCommon / min(num1, num2)\n",
    "    except ZeroDivisionError:\n",
    "        divMin = -1\n",
    "    try:\n",
    "        divProd = numCommon / (num1*num2)\n",
    "    except:\n",
    "        divProd = -1\n",
    "    #retList is of format: num results for str1, num results for str2, num results in common, the Jaccard index,\n",
    "    #the intersection / the min, and the intersection / the product\n",
    "    retList = [num1, num2, numCommon, jacInd, divMin, divProd]\n",
    "    return retList\n",
    "\n",
    "print('done')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import urllib2\n",
    "\n",
    "#choose the first of the following two lines if it's the very start / origination of the file. Otherwise go with second\n",
    "dfRes = pd.DataFrame(columns = [\"str1\", \"str2\", \"num_corecips\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"var1\", \"var2\"])\n",
    "#dfRes = pd.read_csv('./co_donor_google_queries_fin.csv')\n",
    "\n",
    "counter = 0\n",
    "#'''\n",
    "#try:\n",
    "for edge in all_edges_to_sample:\n",
    "    print(counter)\n",
    "    counter = counter + 1\n",
    "    name1 = '\"' + edge[0] + '\"'\n",
    "    name2 = '\"' + edge[1] + '\"'\n",
    "\n",
    "    res = jaccard_google_search(name1, name2, my_api_key, my_cse_id, num=1)#note that the property is called codonors\n",
    "    #so that it's the same as in the other graph, but it actually represents number of corecipients.\n",
    "    dfRow = pd.DataFrame([[edge[0], edge[1], (edge[2])['number_of_codonors'], res[0], res[1], res[2], res[3], res[4], res[5]]], \n",
    "                       columns = [\"str1\", \"str2\", \"num_corecips\", \"num1\", \"num2\", \"numCommon\", \"jacInd\", \"var1\", \"var2\"])\n",
    "\n",
    "    dfRes = dfRes.append(dfRow, ignore_index=True)\n",
    "#'''\n",
    "\n",
    "dfRes.to_csv('./co_recip_google_queries_fin_mac2_1.csv')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfRes.to_csv('./co_recip_google_queries_fin_1.csv')\n",
    "#print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
